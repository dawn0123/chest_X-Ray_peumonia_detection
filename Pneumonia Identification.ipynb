{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andromeda\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir, makedirs\n",
    "from os.path import join, exists, expanduser\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import optimizers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# from keras.applications import vgg16, resnet50, mobilenet\n",
    "# from keras.applications.vgg16 import VGG16\n",
    "# from keras.applications.resnet50 import ResNet50\n",
    "# from keras.applications import xception\n",
    "# from keras.applications import inception_v3\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pred(preds, Y, val_breed, index, seq, ran):\n",
    "    leng = len(preds)\n",
    "    if seq:\n",
    "        for i in range(index):\n",
    "            if ran:\n",
    "                index = random.randint(0, leng) \n",
    "            _, imagenet_class_name, prob = decode_predictions(preds, top=1)[index][0]\n",
    "            plt.title(\"Original: \" + val_breed[Y[index]] + \"\\nPrediction: \" + imagenet_class_name)\n",
    "            plt.imshow(X_train[index])\n",
    "            plt.show()\n",
    "    else:\n",
    "            _, imagenet_class_name, prob = decode_predictions(preds, top=1)[index][0]\n",
    "            plt.title(\"Original: \" + val_breed[Y[index]] + \"\\nPrediction: \" + imagenet_class_name)\n",
    "            plt.imshow(X_train[index])\n",
    "            plt.show()\n",
    "        \n",
    "def accuracy_func(preds, Y, val_breed):\n",
    "    leng = len(preds)\n",
    "    count = 0;\n",
    "    for i in range(leng):\n",
    "        _, imagenet_class_name, prob = decode_predictions(preds, top=1)[i][0]\n",
    "        if val_breed[Y[i]] == imagenet_class_name:\n",
    "            count+=1\n",
    "    accuracy = (count/leng)*100\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = expanduser(join('~', '.keras'))\n",
    "if not exists(cache_dir):\n",
    "    makedirs(cache_dir)\n",
    "models_dir = join(cache_dir, 'models')\n",
    "if not exists(models_dir):\n",
    "    makedirs(models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load the VGG model\n",
    "# vgg_model = vgg16.VGG16(weights='imagenet')\n",
    " \n",
    "# #Load the Inception_V3 model\n",
    "# inception_model = inception_v3.InceptionV3(weights='imagenet')\n",
    " \n",
    "# #Load the ResNet50 model\n",
    "# resnet_model = resnet50.ResNet50(weights='imagenet')\n",
    " \n",
    "# #Load the MobileNet model\n",
    "# mobilenet_model = mobilenet.MobileNet(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = 'data/train'\n",
    "validation_path = 'data/val'\n",
    "testing_path = 'data/test'\n",
    "batch_size = 32\n",
    "target_size=(224, 224)\n",
    "norm = 255.0\n",
    "class_mode='categorical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 16 images belonging to 2 classes.\n",
      "Found 624 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./norm,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./norm)\n",
    "test_datagen = ImageDataGenerator(rescale=1./norm)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        training_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=class_mode)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=class_mode)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        testing_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=class_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(2, activation='softmax')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_1\n",
      "1 conv2d_1\n",
      "2 batch_normalization_1\n",
      "3 activation_1\n",
      "4 conv2d_2\n",
      "5 batch_normalization_2\n",
      "6 activation_2\n",
      "7 conv2d_3\n",
      "8 batch_normalization_3\n",
      "9 activation_3\n",
      "10 max_pooling2d_1\n",
      "11 conv2d_4\n",
      "12 batch_normalization_4\n",
      "13 activation_4\n",
      "14 conv2d_5\n",
      "15 batch_normalization_5\n",
      "16 activation_5\n",
      "17 max_pooling2d_2\n",
      "18 conv2d_9\n",
      "19 batch_normalization_9\n",
      "20 activation_9\n",
      "21 conv2d_7\n",
      "22 conv2d_10\n",
      "23 batch_normalization_7\n",
      "24 batch_normalization_10\n",
      "25 activation_7\n",
      "26 activation_10\n",
      "27 average_pooling2d_1\n",
      "28 conv2d_6\n",
      "29 conv2d_8\n",
      "30 conv2d_11\n",
      "31 conv2d_12\n",
      "32 batch_normalization_6\n",
      "33 batch_normalization_8\n",
      "34 batch_normalization_11\n",
      "35 batch_normalization_12\n",
      "36 activation_6\n",
      "37 activation_8\n",
      "38 activation_11\n",
      "39 activation_12\n",
      "40 mixed0\n",
      "41 conv2d_16\n",
      "42 batch_normalization_16\n",
      "43 activation_16\n",
      "44 conv2d_14\n",
      "45 conv2d_17\n",
      "46 batch_normalization_14\n",
      "47 batch_normalization_17\n",
      "48 activation_14\n",
      "49 activation_17\n",
      "50 average_pooling2d_2\n",
      "51 conv2d_13\n",
      "52 conv2d_15\n",
      "53 conv2d_18\n",
      "54 conv2d_19\n",
      "55 batch_normalization_13\n",
      "56 batch_normalization_15\n",
      "57 batch_normalization_18\n",
      "58 batch_normalization_19\n",
      "59 activation_13\n",
      "60 activation_15\n",
      "61 activation_18\n",
      "62 activation_19\n",
      "63 mixed1\n",
      "64 conv2d_23\n",
      "65 batch_normalization_23\n",
      "66 activation_23\n",
      "67 conv2d_21\n",
      "68 conv2d_24\n",
      "69 batch_normalization_21\n",
      "70 batch_normalization_24\n",
      "71 activation_21\n",
      "72 activation_24\n",
      "73 average_pooling2d_3\n",
      "74 conv2d_20\n",
      "75 conv2d_22\n",
      "76 conv2d_25\n",
      "77 conv2d_26\n",
      "78 batch_normalization_20\n",
      "79 batch_normalization_22\n",
      "80 batch_normalization_25\n",
      "81 batch_normalization_26\n",
      "82 activation_20\n",
      "83 activation_22\n",
      "84 activation_25\n",
      "85 activation_26\n",
      "86 mixed2\n",
      "87 conv2d_28\n",
      "88 batch_normalization_28\n",
      "89 activation_28\n",
      "90 conv2d_29\n",
      "91 batch_normalization_29\n",
      "92 activation_29\n",
      "93 conv2d_27\n",
      "94 conv2d_30\n",
      "95 batch_normalization_27\n",
      "96 batch_normalization_30\n",
      "97 activation_27\n",
      "98 activation_30\n",
      "99 max_pooling2d_3\n",
      "100 mixed3\n",
      "101 conv2d_35\n",
      "102 batch_normalization_35\n",
      "103 activation_35\n",
      "104 conv2d_36\n",
      "105 batch_normalization_36\n",
      "106 activation_36\n",
      "107 conv2d_32\n",
      "108 conv2d_37\n",
      "109 batch_normalization_32\n",
      "110 batch_normalization_37\n",
      "111 activation_32\n",
      "112 activation_37\n",
      "113 conv2d_33\n",
      "114 conv2d_38\n",
      "115 batch_normalization_33\n",
      "116 batch_normalization_38\n",
      "117 activation_33\n",
      "118 activation_38\n",
      "119 average_pooling2d_4\n",
      "120 conv2d_31\n",
      "121 conv2d_34\n",
      "122 conv2d_39\n",
      "123 conv2d_40\n",
      "124 batch_normalization_31\n",
      "125 batch_normalization_34\n",
      "126 batch_normalization_39\n",
      "127 batch_normalization_40\n",
      "128 activation_31\n",
      "129 activation_34\n",
      "130 activation_39\n",
      "131 activation_40\n",
      "132 mixed4\n",
      "133 conv2d_45\n",
      "134 batch_normalization_45\n",
      "135 activation_45\n",
      "136 conv2d_46\n",
      "137 batch_normalization_46\n",
      "138 activation_46\n",
      "139 conv2d_42\n",
      "140 conv2d_47\n",
      "141 batch_normalization_42\n",
      "142 batch_normalization_47\n",
      "143 activation_42\n",
      "144 activation_47\n",
      "145 conv2d_43\n",
      "146 conv2d_48\n",
      "147 batch_normalization_43\n",
      "148 batch_normalization_48\n",
      "149 activation_43\n",
      "150 activation_48\n",
      "151 average_pooling2d_5\n",
      "152 conv2d_41\n",
      "153 conv2d_44\n",
      "154 conv2d_49\n",
      "155 conv2d_50\n",
      "156 batch_normalization_41\n",
      "157 batch_normalization_44\n",
      "158 batch_normalization_49\n",
      "159 batch_normalization_50\n",
      "160 activation_41\n",
      "161 activation_44\n",
      "162 activation_49\n",
      "163 activation_50\n",
      "164 mixed5\n",
      "165 conv2d_55\n",
      "166 batch_normalization_55\n",
      "167 activation_55\n",
      "168 conv2d_56\n",
      "169 batch_normalization_56\n",
      "170 activation_56\n",
      "171 conv2d_52\n",
      "172 conv2d_57\n",
      "173 batch_normalization_52\n",
      "174 batch_normalization_57\n",
      "175 activation_52\n",
      "176 activation_57\n",
      "177 conv2d_53\n",
      "178 conv2d_58\n",
      "179 batch_normalization_53\n",
      "180 batch_normalization_58\n",
      "181 activation_53\n",
      "182 activation_58\n",
      "183 average_pooling2d_6\n",
      "184 conv2d_51\n",
      "185 conv2d_54\n",
      "186 conv2d_59\n",
      "187 conv2d_60\n",
      "188 batch_normalization_51\n",
      "189 batch_normalization_54\n",
      "190 batch_normalization_59\n",
      "191 batch_normalization_60\n",
      "192 activation_51\n",
      "193 activation_54\n",
      "194 activation_59\n",
      "195 activation_60\n",
      "196 mixed6\n",
      "197 conv2d_65\n",
      "198 batch_normalization_65\n",
      "199 activation_65\n",
      "200 conv2d_66\n",
      "201 batch_normalization_66\n",
      "202 activation_66\n",
      "203 conv2d_62\n",
      "204 conv2d_67\n",
      "205 batch_normalization_62\n",
      "206 batch_normalization_67\n",
      "207 activation_62\n",
      "208 activation_67\n",
      "209 conv2d_63\n",
      "210 conv2d_68\n",
      "211 batch_normalization_63\n",
      "212 batch_normalization_68\n",
      "213 activation_63\n",
      "214 activation_68\n",
      "215 average_pooling2d_7\n",
      "216 conv2d_61\n",
      "217 conv2d_64\n",
      "218 conv2d_69\n",
      "219 conv2d_70\n",
      "220 batch_normalization_61\n",
      "221 batch_normalization_64\n",
      "222 batch_normalization_69\n",
      "223 batch_normalization_70\n",
      "224 activation_61\n",
      "225 activation_64\n",
      "226 activation_69\n",
      "227 activation_70\n",
      "228 mixed7\n",
      "229 conv2d_73\n",
      "230 batch_normalization_73\n",
      "231 activation_73\n",
      "232 conv2d_74\n",
      "233 batch_normalization_74\n",
      "234 activation_74\n",
      "235 conv2d_71\n",
      "236 conv2d_75\n",
      "237 batch_normalization_71\n",
      "238 batch_normalization_75\n",
      "239 activation_71\n",
      "240 activation_75\n",
      "241 conv2d_72\n",
      "242 conv2d_76\n",
      "243 batch_normalization_72\n",
      "244 batch_normalization_76\n",
      "245 activation_72\n",
      "246 activation_76\n",
      "247 max_pooling2d_4\n",
      "248 mixed8\n",
      "249 conv2d_81\n",
      "250 batch_normalization_81\n",
      "251 activation_81\n",
      "252 conv2d_78\n",
      "253 conv2d_82\n",
      "254 batch_normalization_78\n",
      "255 batch_normalization_82\n",
      "256 activation_78\n",
      "257 activation_82\n",
      "258 conv2d_79\n",
      "259 conv2d_80\n",
      "260 conv2d_83\n",
      "261 conv2d_84\n",
      "262 average_pooling2d_8\n",
      "263 conv2d_77\n",
      "264 batch_normalization_79\n",
      "265 batch_normalization_80\n",
      "266 batch_normalization_83\n",
      "267 batch_normalization_84\n",
      "268 conv2d_85\n",
      "269 batch_normalization_77\n",
      "270 activation_79\n",
      "271 activation_80\n",
      "272 activation_83\n",
      "273 activation_84\n",
      "274 batch_normalization_85\n",
      "275 activation_77\n",
      "276 mixed9_0\n",
      "277 concatenate_1\n",
      "278 activation_85\n",
      "279 mixed9\n",
      "280 conv2d_90\n",
      "281 batch_normalization_90\n",
      "282 activation_90\n",
      "283 conv2d_87\n",
      "284 conv2d_91\n",
      "285 batch_normalization_87\n",
      "286 batch_normalization_91\n",
      "287 activation_87\n",
      "288 activation_91\n",
      "289 conv2d_88\n",
      "290 conv2d_89\n",
      "291 conv2d_92\n",
      "292 conv2d_93\n",
      "293 average_pooling2d_9\n",
      "294 conv2d_86\n",
      "295 batch_normalization_88\n",
      "296 batch_normalization_89\n",
      "297 batch_normalization_92\n",
      "298 batch_normalization_93\n",
      "299 conv2d_94\n",
      "300 batch_normalization_86\n",
      "301 activation_88\n",
      "302 activation_89\n",
      "303 activation_92\n",
      "304 activation_93\n",
      "305 batch_normalization_94\n",
      "306 activation_86\n",
      "307 mixed9_1\n",
      "308 concatenate_2\n",
      "309 activation_94\n",
      "310 mixed10\n"
     ]
    }
   ],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "\n",
    "\n",
    "\n",
    "sgd = optimizers.Adam()\n",
    "# sgd = optimizers.SGD()\n",
    "# sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# sgd = optimizer=SGD(lr=0.0001, momentum=0.9)\n",
    "\n",
    "model.compile(sgd, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "# model.compile(loss='mean_squared_error', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'outputs/models/'\n",
    "log_file = \"outputs/logs\"\n",
    "\n",
    "model_file = model_dir+\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(model_file, monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = keras.callbacks.TensorBoard(log_dir=log_file, histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)\n",
    "tensorboard.set_model(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "callbacks_list = [checkpoint, tensorboard, early_stopping]\n",
    "# callbacks_list = [checkpoint, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "163/163 [==============================] - ETA: 49:25 - loss: 0.6451 - acc: 0.71 - ETA: 42:53 - loss: 0.7946 - acc: 0.68 - ETA: 40:52 - loss: 1.1742 - acc: 0.58 - ETA: 37:43 - loss: 0.9037 - acc: 0.67 - ETA: 36:00 - loss: 0.7315 - acc: 0.73 - ETA: 34:57 - loss: 0.7148 - acc: 0.75 - ETA: 34:22 - loss: 0.7961 - acc: 0.76 - ETA: 33:54 - loss: 0.8010 - acc: 0.78 - ETA: 33:38 - loss: 0.7438 - acc: 0.79 - ETA: 32:51 - loss: 0.7128 - acc: 0.80 - ETA: 32:14 - loss: 0.6614 - acc: 0.81 - ETA: 31:30 - loss: 0.6188 - acc: 0.82 - ETA: 30:53 - loss: 0.5939 - acc: 0.82 - ETA: 30:20 - loss: 0.5855 - acc: 0.83 - ETA: 29:50 - loss: 0.5549 - acc: 0.83 - ETA: 29:22 - loss: 0.5637 - acc: 0.83 - ETA: 28:55 - loss: 0.5387 - acc: 0.84 - ETA: 28:30 - loss: 0.5144 - acc: 0.84 - ETA: 28:07 - loss: 0.5136 - acc: 0.84 - ETA: 27:45 - loss: 0.5303 - acc: 0.84 - ETA: 27:27 - loss: 0.5103 - acc: 0.85 - ETA: 27:08 - loss: 0.4925 - acc: 0.85 - ETA: 26:48 - loss: 0.4789 - acc: 0.86 - ETA: 26:31 - loss: 0.4687 - acc: 0.86 - ETA: 26:14 - loss: 0.4573 - acc: 0.86 - ETA: 25:56 - loss: 0.4464 - acc: 0.87 - ETA: 25:40 - loss: 0.4410 - acc: 0.87 - ETA: 25:24 - loss: 0.4310 - acc: 0.87 - ETA: 25:08 - loss: 0.4195 - acc: 0.87 - ETA: 24:53 - loss: 0.4146 - acc: 0.87 - ETA: 24:39 - loss: 0.4065 - acc: 0.87 - ETA: 24:25 - loss: 0.3973 - acc: 0.87 - ETA: 24:10 - loss: 0.3886 - acc: 0.88 - ETA: 23:57 - loss: 0.3803 - acc: 0.88 - ETA: 23:42 - loss: 0.3722 - acc: 0.88 - ETA: 23:28 - loss: 0.3693 - acc: 0.88 - ETA: 23:14 - loss: 0.3655 - acc: 0.89 - ETA: 23:00 - loss: 0.3598 - acc: 0.89 - ETA: 22:47 - loss: 0.3559 - acc: 0.89 - ETA: 22:33 - loss: 0.3520 - acc: 0.89 - ETA: 22:21 - loss: 0.3459 - acc: 0.89 - ETA: 22:09 - loss: 0.3420 - acc: 0.89 - ETA: 21:55 - loss: 0.3448 - acc: 0.89 - ETA: 21:44 - loss: 0.3393 - acc: 0.89 - ETA: 21:32 - loss: 0.3340 - acc: 0.89 - ETA: 21:19 - loss: 0.3279 - acc: 0.89 - ETA: 21:07 - loss: 0.3252 - acc: 0.90 - ETA: 20:55 - loss: 0.3205 - acc: 0.90 - ETA: 20:42 - loss: 0.3167 - acc: 0.90 - ETA: 20:30 - loss: 0.3142 - acc: 0.90 - ETA: 20:17 - loss: 0.3087 - acc: 0.90 - ETA: 20:05 - loss: 0.3047 - acc: 0.90 - ETA: 19:53 - loss: 0.3044 - acc: 0.90 - ETA: 19:41 - loss: 0.3012 - acc: 0.90 - ETA: 19:30 - loss: 0.2988 - acc: 0.90 - ETA: 19:18 - loss: 0.2946 - acc: 0.90 - ETA: 19:07 - loss: 0.2924 - acc: 0.90 - ETA: 18:55 - loss: 0.2920 - acc: 0.90 - ETA: 18:44 - loss: 0.2895 - acc: 0.90 - ETA: 18:32 - loss: 0.2858 - acc: 0.90 - ETA: 18:20 - loss: 0.2819 - acc: 0.91 - ETA: 18:09 - loss: 0.2783 - acc: 0.91 - ETA: 17:57 - loss: 0.2756 - acc: 0.91 - ETA: 17:46 - loss: 0.2751 - acc: 0.91 - ETA: 17:35 - loss: 0.2731 - acc: 0.91 - ETA: 17:23 - loss: 0.2701 - acc: 0.91 - ETA: 17:12 - loss: 0.2670 - acc: 0.91 - ETA: 17:02 - loss: 0.2645 - acc: 0.91 - ETA: 16:50 - loss: 0.2626 - acc: 0.91 - ETA: 16:40 - loss: 0.2601 - acc: 0.91 - ETA: 16:28 - loss: 0.2570 - acc: 0.91 - ETA: 16:17 - loss: 0.2563 - acc: 0.91 - ETA: 16:05 - loss: 0.2577 - acc: 0.91 - ETA: 15:54 - loss: 0.2550 - acc: 0.91 - ETA: 15:43 - loss: 0.2547 - acc: 0.91 - ETA: 15:32 - loss: 0.2531 - acc: 0.91 - ETA: 15:20 - loss: 0.2517 - acc: 0.92 - ETA: 15:10 - loss: 0.2502 - acc: 0.92 - ETA: 14:59 - loss: 0.2482 - acc: 0.92 - ETA: 14:47 - loss: 0.2484 - acc: 0.92 - ETA: 14:37 - loss: 0.2461 - acc: 0.92 - ETA: 14:26 - loss: 0.2465 - acc: 0.92 - ETA: 14:15 - loss: 0.2454 - acc: 0.92 - ETA: 14:03 - loss: 0.2455 - acc: 0.92 - ETA: 13:53 - loss: 0.2437 - acc: 0.92 - ETA: 13:41 - loss: 0.2435 - acc: 0.92 - ETA: 13:30 - loss: 0.2433 - acc: 0.92 - ETA: 13:19 - loss: 0.2421 - acc: 0.92 - ETA: 13:08 - loss: 0.2435 - acc: 0.91 - ETA: 12:58 - loss: 0.2422 - acc: 0.92 - ETA: 12:47 - loss: 0.2419 - acc: 0.92 - ETA: 12:36 - loss: 0.2419 - acc: 0.92 - ETA: 12:25 - loss: 0.2406 - acc: 0.92 - ETA: 12:14 - loss: 0.2402 - acc: 0.92 - ETA: 12:03 - loss: 0.2394 - acc: 0.92 - ETA: 11:52 - loss: 0.2392 - acc: 0.92 - ETA: 11:41 - loss: 0.2382 - acc: 0.92 - ETA: 11:30 - loss: 0.2382 - acc: 0.92 - ETA: 11:20 - loss: 0.2368 - acc: 0.92 - ETA: 11:09 - loss: 0.2357 - acc: 0.92 - ETA: 10:58 - loss: 0.2342 - acc: 0.92 - ETA: 10:48 - loss: 0.2328 - acc: 0.92 - ETA: 10:37 - loss: 0.2310 - acc: 0.92 - ETA: 10:26 - loss: 0.2298 - acc: 0.92 - ETA: 10:15 - loss: 0.2279 - acc: 0.92 - ETA: 10:04 - loss: 0.2268 - acc: 0.92 - ETA: 9:54 - loss: 0.2262 - acc: 0.9249 - ETA: 9:43 - loss: 0.2260 - acc: 0.925 - ETA: 9:32 - loss: 0.2265 - acc: 0.925 - ETA: 9:21 - loss: 0.2249 - acc: 0.925 - ETA: 9:10 - loss: 0.2231 - acc: 0.926 - ETA: 9:00 - loss: 0.2223 - acc: 0.926 - ETA: 8:49 - loss: 0.2222 - acc: 0.926 - ETA: 8:38 - loss: 0.2211 - acc: 0.926 - ETA: 8:28 - loss: 0.2201 - acc: 0.926 - ETA: 8:17 - loss: 0.2191 - acc: 0.927 - ETA: 8:06 - loss: 0.2200 - acc: 0.926 - ETA: 7:56 - loss: 0.2185 - acc: 0.927 - ETA: 7:45 - loss: 0.2172 - acc: 0.927 - ETA: 7:34 - loss: 0.2168 - acc: 0.927 - ETA: 7:24 - loss: 0.2165 - acc: 0.927 - ETA: 7:13 - loss: 0.2165 - acc: 0.927 - ETA: 7:02 - loss: 0.2152 - acc: 0.928 - ETA: 6:52 - loss: 0.2148 - acc: 0.928 - ETA: 6:41 - loss: 0.2134 - acc: 0.928 - ETA: 6:30 - loss: 0.2128 - acc: 0.928 - ETA: 6:20 - loss: 0.2119 - acc: 0.928 - ETA: 6:09 - loss: 0.2109 - acc: 0.929 - ETA: 5:58 - loss: 0.2095 - acc: 0.929 - ETA: 5:48 - loss: 0.2086 - acc: 0.929 - ETA: 5:37 - loss: 0.2086 - acc: 0.929 - ETA: 5:27 - loss: 0.2076 - acc: 0.930 - ETA: 5:16 - loss: 0.2067 - acc: 0.930 - ETA: 5:05 - loss: 0.2066 - acc: 0.930 - ETA: 4:55 - loss: 0.2058 - acc: 0.930 - ETA: 4:44 - loss: 0.2051 - acc: 0.930 - ETA: 4:34 - loss: 0.2054 - acc: 0.930 - ETA: 4:23 - loss: 0.2042 - acc: 0.931 - ETA: 4:12 - loss: 0.2029 - acc: 0.931 - ETA: 4:02 - loss: 0.2018 - acc: 0.932 - ETA: 3:51 - loss: 0.2013 - acc: 0.932 - ETA: 3:41 - loss: 0.2002 - acc: 0.932 - ETA: 3:30 - loss: 0.2003 - acc: 0.932 - ETA: 3:20 - loss: 0.2002 - acc: 0.932 - ETA: 3:09 - loss: 0.1997 - acc: 0.933 - ETA: 2:59 - loss: 0.1987 - acc: 0.933 - ETA: 2:48 - loss: 0.1975 - acc: 0.933 - ETA: 2:38 - loss: 0.1993 - acc: 0.933 - ETA: 2:28 - loss: 0.1992 - acc: 0.933 - ETA: 2:17 - loss: 0.1986 - acc: 0.933 - ETA: 2:07 - loss: 0.1990 - acc: 0.933 - ETA: 1:56 - loss: 0.1992 - acc: 0.933 - ETA: 1:46 - loss: 0.1981 - acc: 0.933 - ETA: 1:35 - loss: 0.1980 - acc: 0.933 - ETA: 1:24 - loss: 0.1976 - acc: 0.933 - ETA: 1:14 - loss: 0.1967 - acc: 0.934 - ETA: 1:03 - loss: 0.1959 - acc: 0.934 - ETA: 53s - loss: 0.1964 - acc: 0.934 - ETA: 42s - loss: 0.1957 - acc: 0.93 - ETA: 32s - loss: 0.1953 - acc: 0.93 - ETA: 21s - loss: 0.1943 - acc: 0.93 - ETA: 10s - loss: 0.1950 - acc: 0.93 - 1759s 11s/step - loss: 0.1944 - acc: 0.9348 - val_loss: 1.5006 - val_acc: 0.5625\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - ETA: 33:43 - loss: 0.0871 - acc: 0.96 - ETA: 32:59 - loss: 0.2087 - acc: 0.95 - ETA: 33:39 - loss: 0.1673 - acc: 0.95 - ETA: 33:35 - loss: 0.1423 - acc: 0.96 - ETA: 34:17 - loss: 0.1537 - acc: 0.95 - ETA: 35:08 - loss: 0.1507 - acc: 0.95 - ETA: 34:50 - loss: 0.1423 - acc: 0.95 - ETA: 34:21 - loss: 0.1306 - acc: 0.96 - ETA: 33:58 - loss: 0.1229 - acc: 0.96 - ETA: 33:20 - loss: 0.1202 - acc: 0.96 - ETA: 32:51 - loss: 0.1210 - acc: 0.96 - ETA: 32:28 - loss: 0.1242 - acc: 0.96 - ETA: 32:14 - loss: 0.1236 - acc: 0.95 - ETA: 31:47 - loss: 0.1230 - acc: 0.95 - ETA: 31:35 - loss: 0.1190 - acc: 0.96 - ETA: 31:29 - loss: 0.1190 - acc: 0.95 - ETA: 31:18 - loss: 0.1174 - acc: 0.95 - ETA: 31:00 - loss: 0.1258 - acc: 0.95 - ETA: 30:40 - loss: 0.1208 - acc: 0.95 - ETA: 30:37 - loss: 0.1322 - acc: 0.95 - ETA: 30:21 - loss: 0.1398 - acc: 0.95 - ETA: 30:26 - loss: 0.1353 - acc: 0.95 - ETA: 30:21 - loss: 0.1303 - acc: 0.95 - ETA: 30:05 - loss: 0.1295 - acc: 0.95 - ETA: 29:54 - loss: 0.1296 - acc: 0.95 - ETA: 29:55 - loss: 0.1333 - acc: 0.95 - ETA: 29:53 - loss: 0.1341 - acc: 0.95 - ETA: 29:50 - loss: 0.1308 - acc: 0.95 - ETA: 29:42 - loss: 0.1289 - acc: 0.95 - ETA: 29:25 - loss: 0.1355 - acc: 0.95 - ETA: 29:23 - loss: 0.1412 - acc: 0.95 - ETA: 29:13 - loss: 0.1386 - acc: 0.95 - ETA: 28:56 - loss: 0.1356 - acc: 0.95 - ETA: 28:55 - loss: 0.1328 - acc: 0.95 - ETA: 28:52 - loss: 0.1386 - acc: 0.95 - ETA: 28:36 - loss: 0.1368 - acc: 0.95 - ETA: 28:18 - loss: 0.1362 - acc: 0.95 - ETA: 28:01 - loss: 0.1343 - acc: 0.95 - ETA: 27:44 - loss: 0.1356 - acc: 0.95 - ETA: 27:31 - loss: 0.1348 - acc: 0.95 - ETA: 27:21 - loss: 0.1336 - acc: 0.95 - ETA: 27:13 - loss: 0.1312 - acc: 0.95 - ETA: 27:04 - loss: 0.1307 - acc: 0.95 - ETA: 26:57 - loss: 0.1292 - acc: 0.95 - ETA: 26:51 - loss: 0.1299 - acc: 0.95 - ETA: 26:36 - loss: 0.1309 - acc: 0.95 - ETA: 26:20 - loss: 0.1319 - acc: 0.95 - ETA: 26:03 - loss: 0.1334 - acc: 0.95 - ETA: 25:53 - loss: 0.1355 - acc: 0.95 - ETA: 25:37 - loss: 0.1353 - acc: 0.95 - ETA: 25:18 - loss: 0.1341 - acc: 0.95 - ETA: 25:01 - loss: 0.1328 - acc: 0.95 - ETA: 24:43 - loss: 0.1320 - acc: 0.95 - ETA: 24:27 - loss: 0.1305 - acc: 0.95 - ETA: 24:13 - loss: 0.1288 - acc: 0.95 - ETA: 24:01 - loss: 0.1273 - acc: 0.95 - ETA: 23:49 - loss: 0.1261 - acc: 0.95 - ETA: 23:34 - loss: 0.1264 - acc: 0.95 - ETA: 23:20 - loss: 0.1250 - acc: 0.95 - ETA: 23:04 - loss: 0.1245 - acc: 0.95 - ETA: 22:48 - loss: 0.1228 - acc: 0.95 - ETA: 22:33 - loss: 0.1213 - acc: 0.95 - ETA: 22:18 - loss: 0.1202 - acc: 0.95 - ETA: 22:03 - loss: 0.1188 - acc: 0.96 - ETA: 21:48 - loss: 0.1180 - acc: 0.96 - ETA: 21:33 - loss: 0.1169 - acc: 0.96 - ETA: 21:18 - loss: 0.1159 - acc: 0.96 - ETA: 21:05 - loss: 0.1163 - acc: 0.96 - ETA: 20:55 - loss: 0.1162 - acc: 0.96 - ETA: 20:41 - loss: 0.1153 - acc: 0.96 - ETA: 20:25 - loss: 0.1140 - acc: 0.96 - ETA: 20:10 - loss: 0.1126 - acc: 0.96 - ETA: 19:54 - loss: 0.1112 - acc: 0.96 - ETA: 19:39 - loss: 0.1098 - acc: 0.96 - ETA: 19:24 - loss: 0.1095 - acc: 0.96 - ETA: 19:09 - loss: 0.1117 - acc: 0.96 - ETA: 18:54 - loss: 0.1107 - acc: 0.96 - ETA: 18:39 - loss: 0.1102 - acc: 0.96 - ETA: 18:25 - loss: 0.1105 - acc: 0.96 - ETA: 18:10 - loss: 0.1097 - acc: 0.96 - ETA: 17:57 - loss: 0.1094 - acc: 0.96 - ETA: 17:42 - loss: 0.1089 - acc: 0.96 - ETA: 17:28 - loss: 0.1089 - acc: 0.96 - ETA: 17:13 - loss: 0.1081 - acc: 0.96 - ETA: 16:59 - loss: 0.1071 - acc: 0.96 - ETA: 16:45 - loss: 0.1062 - acc: 0.96 - ETA: 16:32 - loss: 0.1076 - acc: 0.96 - ETA: 16:18 - loss: 0.1079 - acc: 0.96 - ETA: 16:05 - loss: 0.1073 - acc: 0.96 - ETA: 15:52 - loss: 0.1066 - acc: 0.96 - ETA: 15:38 - loss: 0.1061 - acc: 0.96 - ETA: 15:25 - loss: 0.1077 - acc: 0.96 - ETA: 15:13 - loss: 0.1068 - acc: 0.96 - ETA: 15:00 - loss: 0.1067 - acc: 0.96 - ETA: 14:47 - loss: 0.1068 - acc: 0.96 - ETA: 14:32 - loss: 0.1072 - acc: 0.96 - ETA: 14:19 - loss: 0.1067 - acc: 0.96 - ETA: 14:05 - loss: 0.1106 - acc: 0.96 - ETA: 13:52 - loss: 0.1103 - acc: 0.96 - ETA: 13:38 - loss: 0.1097 - acc: 0.96 - ETA: 13:24 - loss: 0.1106 - acc: 0.96 - ETA: 13:10 - loss: 0.1124 - acc: 0.96 - ETA: 12:58 - loss: 0.1139 - acc: 0.96 - ETA: 12:44 - loss: 0.1139 - acc: 0.96 - ETA: 12:30 - loss: 0.1140 - acc: 0.96 - ETA: 12:17 - loss: 0.1148 - acc: 0.96 - ETA: 12:03 - loss: 0.1144 - acc: 0.96 - ETA: 11:50 - loss: 0.1139 - acc: 0.96 - ETA: 11:36 - loss: 0.1135 - acc: 0.96 - ETA: 11:23 - loss: 0.1137 - acc: 0.96 - ETA: 11:10 - loss: 0.1132 - acc: 0.96 - ETA: 10:56 - loss: 0.1148 - acc: 0.96 - ETA: 10:43 - loss: 0.1144 - acc: 0.96 - ETA: 10:29 - loss: 0.1141 - acc: 0.96 - ETA: 10:16 - loss: 0.1150 - acc: 0.96 - ETA: 10:03 - loss: 0.1140 - acc: 0.96 - ETA: 9:49 - loss: 0.1132 - acc: 0.9621 - ETA: 9:36 - loss: 0.1136 - acc: 0.961 - ETA: 9:23 - loss: 0.1140 - acc: 0.961 - ETA: 9:10 - loss: 0.1151 - acc: 0.961 - ETA: 8:57 - loss: 0.1144 - acc: 0.961 - ETA: 8:44 - loss: 0.1136 - acc: 0.961 - ETA: 8:31 - loss: 0.1139 - acc: 0.961 - ETA: 8:18 - loss: 0.1138 - acc: 0.961 - ETA: 8:05 - loss: 0.1157 - acc: 0.961 - ETA: 7:52 - loss: 0.1152 - acc: 0.961 - ETA: 7:39 - loss: 0.1163 - acc: 0.960 - ETA: 7:26 - loss: 0.1160 - acc: 0.960 - ETA: 7:13 - loss: 0.1157 - acc: 0.960 - ETA: 7:01 - loss: 0.1157 - acc: 0.960 - ETA: 6:49 - loss: 0.1152 - acc: 0.960 - ETA: 6:37 - loss: 0.1151 - acc: 0.960 - ETA: 6:24 - loss: 0.1147 - acc: 0.961 - ETA: 6:11 - loss: 0.1140 - acc: 0.961 - ETA: 5:58 - loss: 0.1140 - acc: 0.961 - ETA: 5:46 - loss: 0.1150 - acc: 0.960 - ETA: 5:33 - loss: 0.1151 - acc: 0.960 - ETA: 5:21 - loss: 0.1144 - acc: 0.960 - ETA: 5:08 - loss: 0.1138 - acc: 0.961 - ETA: 4:55 - loss: 0.1134 - acc: 0.961 - ETA: 4:42 - loss: 0.1128 - acc: 0.961 - ETA: 4:29 - loss: 0.1126 - acc: 0.961 - ETA: 4:17 - loss: 0.1126 - acc: 0.961 - ETA: 4:04 - loss: 0.1121 - acc: 0.961 - ETA: 3:51 - loss: 0.1117 - acc: 0.961 - ETA: 3:38 - loss: 0.1113 - acc: 0.961 - ETA: 3:26 - loss: 0.1105 - acc: 0.961 - ETA: 3:12 - loss: 0.1111 - acc: 0.961 - ETA: 3:00 - loss: 0.1123 - acc: 0.961 - ETA: 2:47 - loss: 0.1129 - acc: 0.960 - ETA: 2:34 - loss: 0.1129 - acc: 0.960 - ETA: 2:21 - loss: 0.1136 - acc: 0.960 - ETA: 2:08 - loss: 0.1141 - acc: 0.960 - ETA: 1:55 - loss: 0.1136 - acc: 0.960 - ETA: 1:42 - loss: 0.1137 - acc: 0.960 - ETA: 1:30 - loss: 0.1133 - acc: 0.960 - ETA: 1:17 - loss: 0.1133 - acc: 0.960 - ETA: 1:04 - loss: 0.1134 - acc: 0.960 - ETA: 51s - loss: 0.1143 - acc: 0.959 - ETA: 38s - loss: 0.1138 - acc: 0.96 - ETA: 25s - loss: 0.1140 - acc: 0.95 - ETA: 12s - loss: 0.1134 - acc: 0.95 - 2104s 13s/step - loss: 0.1134 - acc: 0.9597 - val_loss: 1.3007 - val_acc: 0.5000\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - ETA: 33:28 - loss: 0.0643 - acc: 1.00 - ETA: 32:38 - loss: 0.0421 - acc: 1.00 - ETA: 31:14 - loss: 0.0454 - acc: 0.98 - ETA: 30:26 - loss: 0.0490 - acc: 0.98 - ETA: 29:55 - loss: 0.0505 - acc: 0.98 - ETA: 29:23 - loss: 0.0732 - acc: 0.97 - ETA: 29:16 - loss: 0.0721 - acc: 0.97 - ETA: 29:34 - loss: 0.0787 - acc: 0.96 - ETA: 29:48 - loss: 0.0817 - acc: 0.96 - ETA: 29:40 - loss: 0.0871 - acc: 0.96 - ETA: 29:41 - loss: 0.0858 - acc: 0.96 - ETA: 29:36 - loss: 0.0828 - acc: 0.96 - ETA: 29:20 - loss: 0.0834 - acc: 0.96 - ETA: 28:58 - loss: 0.0786 - acc: 0.97 - ETA: 28:42 - loss: 0.0753 - acc: 0.97 - ETA: 28:27 - loss: 0.0750 - acc: 0.97 - ETA: 28:10 - loss: 0.0769 - acc: 0.97 - ETA: 27:57 - loss: 0.0754 - acc: 0.97 - ETA: 27:38 - loss: 0.0846 - acc: 0.96 - ETA: 27:31 - loss: 0.0830 - acc: 0.97 - ETA: 27:18 - loss: 0.0887 - acc: 0.97 - ETA: 27:05 - loss: 0.0870 - acc: 0.97 - ETA: 26:55 - loss: 0.0884 - acc: 0.96 - ETA: 26:44 - loss: 0.0887 - acc: 0.96 - ETA: 26:30 - loss: 0.0869 - acc: 0.96 - ETA: 26:10 - loss: 0.0853 - acc: 0.96 - ETA: 25:53 - loss: 0.0889 - acc: 0.96 - ETA: 25:37 - loss: 0.0872 - acc: 0.96 - ETA: 25:20 - loss: 0.0856 - acc: 0.96 - ETA: 25:04 - loss: 0.0834 - acc: 0.96 - ETA: 24:49 - loss: 0.0853 - acc: 0.96 - ETA: 24:33 - loss: 0.0844 - acc: 0.96 - ETA: 24:18 - loss: 0.0858 - acc: 0.96 - ETA: 24:03 - loss: 0.0848 - acc: 0.96 - ETA: 23:49 - loss: 0.0833 - acc: 0.96 - ETA: 23:35 - loss: 0.0822 - acc: 0.97 - ETA: 23:20 - loss: 0.0858 - acc: 0.96 - ETA: 23:07 - loss: 0.0845 - acc: 0.97 - ETA: 22:53 - loss: 0.0828 - acc: 0.97 - ETA: 22:40 - loss: 0.0824 - acc: 0.97 - ETA: 22:28 - loss: 0.0821 - acc: 0.97 - ETA: 22:16 - loss: 0.0828 - acc: 0.96 - ETA: 22:02 - loss: 0.0819 - acc: 0.96 - ETA: 21:49 - loss: 0.0806 - acc: 0.97 - ETA: 21:37 - loss: 0.0801 - acc: 0.97 - ETA: 21:24 - loss: 0.0812 - acc: 0.96 - ETA: 21:12 - loss: 0.0842 - acc: 0.96 - ETA: 20:59 - loss: 0.0840 - acc: 0.96 - ETA: 20:46 - loss: 0.0919 - acc: 0.96 - ETA: 20:34 - loss: 0.0921 - acc: 0.96 - ETA: 20:21 - loss: 0.0919 - acc: 0.96 - ETA: 20:09 - loss: 0.0911 - acc: 0.96 - ETA: 19:57 - loss: 0.0942 - acc: 0.96 - ETA: 19:45 - loss: 0.0957 - acc: 0.96 - ETA: 19:33 - loss: 0.0950 - acc: 0.96 - ETA: 19:21 - loss: 0.0941 - acc: 0.96 - ETA: 19:09 - loss: 0.0942 - acc: 0.96 - ETA: 18:57 - loss: 0.0937 - acc: 0.96 - ETA: 18:46 - loss: 0.0945 - acc: 0.96 - ETA: 18:34 - loss: 0.0946 - acc: 0.96 - ETA: 18:22 - loss: 0.0938 - acc: 0.96 - ETA: 18:12 - loss: 0.0949 - acc: 0.96 - ETA: 18:00 - loss: 0.0945 - acc: 0.96 - ETA: 17:49 - loss: 0.0936 - acc: 0.96 - ETA: 17:38 - loss: 0.0942 - acc: 0.96 - ETA: 17:26 - loss: 0.0950 - acc: 0.96 - ETA: 17:17 - loss: 0.0953 - acc: 0.96 - ETA: 17:07 - loss: 0.0965 - acc: 0.96 - ETA: 16:56 - loss: 0.0970 - acc: 0.96 - ETA: 16:45 - loss: 0.0972 - acc: 0.96 - ETA: 16:35 - loss: 0.0965 - acc: 0.96 - ETA: 16:24 - loss: 0.0974 - acc: 0.96 - ETA: 16:14 - loss: 0.0962 - acc: 0.96 - ETA: 16:03 - loss: 0.0959 - acc: 0.96 - ETA: 15:52 - loss: 0.0971 - acc: 0.96 - ETA: 15:41 - loss: 0.0969 - acc: 0.96 - ETA: 15:31 - loss: 0.0967 - acc: 0.96 - ETA: 15:21 - loss: 0.0964 - acc: 0.96 - ETA: 15:10 - loss: 0.0957 - acc: 0.96 - ETA: 15:00 - loss: 0.0949 - acc: 0.96 - ETA: 14:49 - loss: 0.0940 - acc: 0.96 - ETA: 14:37 - loss: 0.0940 - acc: 0.96 - ETA: 14:26 - loss: 0.0982 - acc: 0.96 - ETA: 14:14 - loss: 0.0973 - acc: 0.96 - ETA: 14:03 - loss: 0.0966 - acc: 0.96 - ETA: 13:51 - loss: 0.0978 - acc: 0.96 - ETA: 13:40 - loss: 0.0973 - acc: 0.96 - ETA: 13:29 - loss: 0.0975 - acc: 0.96 - ETA: 13:18 - loss: 0.0974 - acc: 0.96 - ETA: 13:06 - loss: 0.0968 - acc: 0.96 - ETA: 12:55 - loss: 0.0978 - acc: 0.96 - ETA: 12:44 - loss: 0.0977 - acc: 0.96 - ETA: 12:33 - loss: 0.0971 - acc: 0.96 - ETA: 12:22 - loss: 0.0983 - acc: 0.96 - ETA: 12:11 - loss: 0.0986 - acc: 0.96 - ETA: 11:59 - loss: 0.0986 - acc: 0.96 - ETA: 11:48 - loss: 0.0989 - acc: 0.96 - ETA: 11:37 - loss: 0.0987 - acc: 0.96 - ETA: 11:26 - loss: 0.0984 - acc: 0.96 - ETA: 11:15 - loss: 0.0984 - acc: 0.96 - ETA: 11:04 - loss: 0.0986 - acc: 0.96 - ETA: 10:53 - loss: 0.0985 - acc: 0.96 - ETA: 10:42 - loss: 0.0990 - acc: 0.96 - ETA: 10:32 - loss: 0.0994 - acc: 0.96 - ETA: 10:21 - loss: 0.0987 - acc: 0.96 - ETA: 10:10 - loss: 0.0980 - acc: 0.96 - ETA: 9:59 - loss: 0.0989 - acc: 0.9655 - ETA: 9:48 - loss: 0.0989 - acc: 0.965 - ETA: 9:37 - loss: 0.0987 - acc: 0.965 - ETA: 9:26 - loss: 0.0979 - acc: 0.965 - ETA: 9:15 - loss: 0.0981 - acc: 0.965 - ETA: 9:04 - loss: 0.0977 - acc: 0.965 - ETA: 8:53 - loss: 0.0982 - acc: 0.965 - ETA: 8:42 - loss: 0.0977 - acc: 0.966 - ETA: 8:31 - loss: 0.0982 - acc: 0.966 - ETA: 8:21 - loss: 0.0976 - acc: 0.966 - ETA: 8:10 - loss: 0.0968 - acc: 0.966 - ETA: 7:59 - loss: 0.0966 - acc: 0.966 - ETA: 7:48 - loss: 0.0963 - acc: 0.966 - ETA: 7:37 - loss: 0.0958 - acc: 0.966 - ETA: 7:27 - loss: 0.0961 - acc: 0.966 - ETA: 7:16 - loss: 0.0959 - acc: 0.966 - ETA: 7:05 - loss: 0.0961 - acc: 0.966 - ETA: 6:54 - loss: 0.0958 - acc: 0.966 - ETA: 6:43 - loss: 0.0952 - acc: 0.966 - ETA: 6:33 - loss: 0.0950 - acc: 0.966 - ETA: 6:22 - loss: 0.0950 - acc: 0.966 - ETA: 6:11 - loss: 0.0944 - acc: 0.966 - ETA: 6:01 - loss: 0.0938 - acc: 0.967 - ETA: 5:50 - loss: 0.0932 - acc: 0.967 - ETA: 5:39 - loss: 0.0928 - acc: 0.967 - ETA: 5:29 - loss: 0.0923 - acc: 0.967 - ETA: 5:18 - loss: 0.0941 - acc: 0.967 - ETA: 5:07 - loss: 0.0944 - acc: 0.967 - ETA: 4:57 - loss: 0.0946 - acc: 0.967 - ETA: 4:46 - loss: 0.0941 - acc: 0.967 - ETA: 4:35 - loss: 0.0939 - acc: 0.967 - ETA: 4:25 - loss: 0.0950 - acc: 0.966 - ETA: 4:14 - loss: 0.0944 - acc: 0.967 - ETA: 4:03 - loss: 0.0939 - acc: 0.967 - ETA: 3:53 - loss: 0.0937 - acc: 0.967 - ETA: 3:42 - loss: 0.0933 - acc: 0.967 - ETA: 3:31 - loss: 0.0942 - acc: 0.967 - ETA: 3:21 - loss: 0.0939 - acc: 0.967 - ETA: 3:10 - loss: 0.0939 - acc: 0.967 - ETA: 2:59 - loss: 0.0935 - acc: 0.967 - ETA: 2:49 - loss: 0.0948 - acc: 0.966 - ETA: 2:38 - loss: 0.0950 - acc: 0.966 - ETA: 2:28 - loss: 0.0945 - acc: 0.966 - ETA: 2:17 - loss: 0.0942 - acc: 0.966 - ETA: 2:06 - loss: 0.0949 - acc: 0.966 - ETA: 1:56 - loss: 0.0948 - acc: 0.966 - ETA: 1:45 - loss: 0.0947 - acc: 0.966 - ETA: 1:35 - loss: 0.0956 - acc: 0.966 - ETA: 1:24 - loss: 0.0959 - acc: 0.966 - ETA: 1:14 - loss: 0.0962 - acc: 0.965 - ETA: 1:03 - loss: 0.0968 - acc: 0.965 - ETA: 53s - loss: 0.0964 - acc: 0.966 - ETA: 42s - loss: 0.0969 - acc: 0.96 - ETA: 31s - loss: 0.0973 - acc: 0.96 - ETA: 21s - loss: 0.0972 - acc: 0.96 - ETA: 10s - loss: 0.0969 - acc: 0.96 - 1745s 11s/step - loss: 0.0967 - acc: 0.9657 - val_loss: 1.2068 - val_acc: 0.6250\n",
      "Epoch 4/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 89/163 [===============>..............] - ETA: 31:16 - loss: 0.1218 - acc: 0.96 - ETA: 29:38 - loss: 0.0816 - acc: 0.98 - ETA: 30:24 - loss: 0.0669 - acc: 0.98 - ETA: 30:37 - loss: 0.0860 - acc: 0.96 - ETA: 30:06 - loss: 0.0845 - acc: 0.96 - ETA: 29:55 - loss: 0.0819 - acc: 0.96 - ETA: 29:35 - loss: 0.0762 - acc: 0.96 - ETA: 29:12 - loss: 0.0747 - acc: 0.96 - ETA: 28:55 - loss: 0.0703 - acc: 0.96 - ETA: 28:48 - loss: 0.0710 - acc: 0.96 - ETA: 28:34 - loss: 0.0684 - acc: 0.97 - ETA: 28:10 - loss: 0.0651 - acc: 0.97 - ETA: 27:52 - loss: 0.0653 - acc: 0.97 - ETA: 27:34 - loss: 0.0707 - acc: 0.97 - ETA: 27:22 - loss: 0.0680 - acc: 0.97 - ETA: 27:11 - loss: 0.0670 - acc: 0.97 - ETA: 26:58 - loss: 0.0696 - acc: 0.97 - ETA: 26:38 - loss: 0.0728 - acc: 0.97 - ETA: 26:22 - loss: 0.0706 - acc: 0.97 - ETA: 26:08 - loss: 0.0681 - acc: 0.97 - ETA: 25:53 - loss: 0.0798 - acc: 0.97 - ETA: 25:38 - loss: 0.0829 - acc: 0.96 - ETA: 25:23 - loss: 0.0798 - acc: 0.96 - ETA: 25:08 - loss: 0.0796 - acc: 0.96 - ETA: 24:56 - loss: 0.0788 - acc: 0.96 - ETA: 24:40 - loss: 0.0798 - acc: 0.96 - ETA: 24:28 - loss: 0.0785 - acc: 0.96 - ETA: 24:16 - loss: 0.0771 - acc: 0.96 - ETA: 24:02 - loss: 0.0756 - acc: 0.96 - ETA: 23:50 - loss: 0.0826 - acc: 0.96 - ETA: 23:38 - loss: 0.0837 - acc: 0.96 - ETA: 23:24 - loss: 0.0845 - acc: 0.96 - ETA: 23:13 - loss: 0.0826 - acc: 0.96 - ETA: 23:01 - loss: 0.0826 - acc: 0.96 - ETA: 22:48 - loss: 0.0809 - acc: 0.96 - ETA: 22:36 - loss: 0.0858 - acc: 0.96 - ETA: 22:24 - loss: 0.0851 - acc: 0.96 - ETA: 22:13 - loss: 0.0832 - acc: 0.96 - ETA: 22:01 - loss: 0.0878 - acc: 0.96 - ETA: 21:49 - loss: 0.0877 - acc: 0.96 - ETA: 21:37 - loss: 0.0857 - acc: 0.96 - ETA: 21:26 - loss: 0.0859 - acc: 0.96 - ETA: 21:14 - loss: 0.0870 - acc: 0.96 - ETA: 21:02 - loss: 0.0882 - acc: 0.96 - ETA: 20:51 - loss: 0.0897 - acc: 0.96 - ETA: 20:39 - loss: 0.0901 - acc: 0.96 - ETA: 20:28 - loss: 0.0916 - acc: 0.96 - ETA: 20:17 - loss: 0.0917 - acc: 0.96 - ETA: 20:06 - loss: 0.0913 - acc: 0.96 - ETA: 19:55 - loss: 0.0919 - acc: 0.96 - ETA: 19:44 - loss: 0.0911 - acc: 0.96 - ETA: 19:33 - loss: 0.0902 - acc: 0.96 - ETA: 19:22 - loss: 0.0892 - acc: 0.96 - ETA: 19:11 - loss: 0.0892 - acc: 0.96 - ETA: 19:00 - loss: 0.0908 - acc: 0.96 - ETA: 18:50 - loss: 0.0915 - acc: 0.96 - ETA: 18:39 - loss: 0.0921 - acc: 0.96 - ETA: 18:28 - loss: 0.0922 - acc: 0.96 - ETA: 18:19 - loss: 0.0910 - acc: 0.96 - ETA: 18:13 - loss: 0.0900 - acc: 0.96 - ETA: 18:06 - loss: 0.0891 - acc: 0.96 - ETA: 17:58 - loss: 0.0899 - acc: 0.96 - ETA: 17:48 - loss: 0.0899 - acc: 0.96 - ETA: 17:39 - loss: 0.0894 - acc: 0.96 - ETA: 17:29 - loss: 0.0881 - acc: 0.96 - ETA: 17:19 - loss: 0.0871 - acc: 0.96 - ETA: 17:09 - loss: 0.0861 - acc: 0.96 - ETA: 16:59 - loss: 0.0863 - acc: 0.96 - ETA: 16:49 - loss: 0.0856 - acc: 0.96 - ETA: 16:40 - loss: 0.0881 - acc: 0.96 - ETA: 16:29 - loss: 0.0883 - acc: 0.96 - ETA: 16:19 - loss: 0.0880 - acc: 0.96 - ETA: 16:09 - loss: 0.0889 - acc: 0.96 - ETA: 15:58 - loss: 0.0898 - acc: 0.96 - ETA: 15:47 - loss: 0.0900 - acc: 0.96 - ETA: 15:36 - loss: 0.0897 - acc: 0.96 - ETA: 15:26 - loss: 0.0893 - acc: 0.96 - ETA: 15:16 - loss: 0.0886 - acc: 0.96 - ETA: 15:06 - loss: 0.0884 - acc: 0.96 - ETA: 14:56 - loss: 0.0886 - acc: 0.96 - ETA: 14:47 - loss: 0.0883 - acc: 0.96 - ETA: 14:37 - loss: 0.0880 - acc: 0.96 - ETA: 14:27 - loss: 0.0885 - acc: 0.96 - ETA: 14:18 - loss: 0.0876 - acc: 0.96 - ETA: 14:08 - loss: 0.0872 - acc: 0.96 - ETA: 13:57 - loss: 0.0867 - acc: 0.96 - ETA: 13:46 - loss: 0.0859 - acc: 0.96 - ETA: 13:36 - loss: 0.0858 - acc: 0.96 - ETA: 13:27 - loss: 0.0868 - acc: 0.9649"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-27-289cb4090804>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_generator\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     callbacks=callbacks_list)\n\u001b[0m",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name +\n\u001b[0;32m     90\u001b[0m                               '` call to the Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(self, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m   1413\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1414\u001b[0m             \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1415\u001b[1;33m             initial_epoch=initial_epoch)\n\u001b[0m\u001b[0;32m   1416\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1417\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mfit_generator\u001b[1;34m(model, generator, steps_per_epoch, epochs, verbose, callbacks, validation_data, validation_steps, class_weight, max_queue_size, workers, use_multiprocessing, shuffle, initial_epoch)\u001b[0m\n\u001b[0;32m    211\u001b[0m                 outs = model.train_on_batch(x, y,\n\u001b[0;32m    212\u001b[0m                                             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 213\u001b[1;33m                                             class_weight=class_weight)\n\u001b[0m\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[1;34m(self, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[0;32m   1213\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1214\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_train_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1215\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1216\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2664\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2665\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2666\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2667\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2668\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2634\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2635\u001b[0m                                 session)\n\u001b[1;32m-> 2636\u001b[1;33m         \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2637\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2638\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1382\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1383\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    "    validation_data=validation_generator,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Test'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'outputs/models/weights-improvement-03-0.62.hdf5'\n",
    "model = keras.models.load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "163/163 [==============================] - ETA: 47:57 - loss: 0.1382 - acc: 0.93 - ETA: 41:37 - loss: 0.1177 - acc: 0.95 - ETA: 37:26 - loss: 0.0812 - acc: 0.96 - ETA: 35:01 - loss: 0.0979 - acc: 0.96 - ETA: 33:28 - loss: 0.0864 - acc: 0.96 - ETA: 32:12 - loss: 0.0855 - acc: 0.96 - ETA: 31:22 - loss: 0.0810 - acc: 0.96 - ETA: 30:43 - loss: 0.0847 - acc: 0.96 - ETA: 30:03 - loss: 0.0947 - acc: 0.96 - ETA: 29:35 - loss: 0.0903 - acc: 0.96 - ETA: 29:10 - loss: 0.0850 - acc: 0.96 - ETA: 28:43 - loss: 0.0803 - acc: 0.97 - ETA: 28:23 - loss: 0.0757 - acc: 0.97 - ETA: 28:04 - loss: 0.0784 - acc: 0.97 - ETA: 27:43 - loss: 0.0808 - acc: 0.96 - ETA: 27:27 - loss: 0.0795 - acc: 0.97 - ETA: 27:12 - loss: 0.0790 - acc: 0.97 - ETA: 26:53 - loss: 0.0772 - acc: 0.97 - ETA: 26:39 - loss: 0.0780 - acc: 0.97 - ETA: 26:25 - loss: 0.0754 - acc: 0.97 - ETA: 26:09 - loss: 0.0907 - acc: 0.96 - ETA: 25:55 - loss: 0.0898 - acc: 0.96 - ETA: 25:42 - loss: 0.0868 - acc: 0.97 - ETA: 25:27 - loss: 0.0837 - acc: 0.97 - ETA: 25:13 - loss: 0.0877 - acc: 0.97 - ETA: 25:00 - loss: 0.0891 - acc: 0.96 - ETA: 24:46 - loss: 0.0941 - acc: 0.96 - ETA: 24:33 - loss: 0.0949 - acc: 0.96 - ETA: 24:20 - loss: 0.0926 - acc: 0.96 - ETA: 24:07 - loss: 0.0910 - acc: 0.96 - ETA: 23:55 - loss: 0.0920 - acc: 0.96 - ETA: 23:42 - loss: 0.0898 - acc: 0.96 - ETA: 23:30 - loss: 0.0881 - acc: 0.96 - ETA: 23:18 - loss: 0.0859 - acc: 0.96 - ETA: 23:06 - loss: 0.0859 - acc: 0.96 - ETA: 22:54 - loss: 0.0865 - acc: 0.96 - ETA: 22:43 - loss: 0.0858 - acc: 0.96 - ETA: 22:30 - loss: 0.0854 - acc: 0.96 - ETA: 22:18 - loss: 0.0857 - acc: 0.96 - ETA: 22:07 - loss: 0.0848 - acc: 0.96 - ETA: 21:55 - loss: 0.0850 - acc: 0.96 - ETA: 21:44 - loss: 0.0838 - acc: 0.96 - ETA: 21:33 - loss: 0.0825 - acc: 0.96 - ETA: 21:21 - loss: 0.0817 - acc: 0.96 - ETA: 21:10 - loss: 0.0803 - acc: 0.97 - ETA: 20:59 - loss: 0.0812 - acc: 0.97 - ETA: 20:47 - loss: 0.0805 - acc: 0.97 - ETA: 20:36 - loss: 0.0805 - acc: 0.97 - ETA: 20:26 - loss: 0.0796 - acc: 0.97 - ETA: 20:14 - loss: 0.0803 - acc: 0.97 - ETA: 20:03 - loss: 0.0805 - acc: 0.97 - ETA: 19:52 - loss: 0.0798 - acc: 0.97 - ETA: 19:40 - loss: 0.0799 - acc: 0.97 - ETA: 19:29 - loss: 0.0801 - acc: 0.97 - ETA: 19:18 - loss: 0.0788 - acc: 0.97 - ETA: 19:07 - loss: 0.0792 - acc: 0.97 - ETA: 18:56 - loss: 0.0787 - acc: 0.97 - ETA: 18:45 - loss: 0.0780 - acc: 0.97 - ETA: 18:33 - loss: 0.0768 - acc: 0.97 - ETA: 18:22 - loss: 0.0758 - acc: 0.97 - ETA: 18:12 - loss: 0.0749 - acc: 0.97 - ETA: 18:01 - loss: 0.0744 - acc: 0.97 - ETA: 17:50 - loss: 0.0751 - acc: 0.97 - ETA: 17:42 - loss: 0.0740 - acc: 0.97 - ETA: 17:34 - loss: 0.0737 - acc: 0.97 - ETA: 17:26 - loss: 0.0739 - acc: 0.97 - ETA: 17:17 - loss: 0.0742 - acc: 0.97 - ETA: 17:07 - loss: 0.0768 - acc: 0.97 - ETA: 16:58 - loss: 0.0761 - acc: 0.97 - ETA: 16:48 - loss: 0.0764 - acc: 0.97 - ETA: 16:39 - loss: 0.0755 - acc: 0.97 - ETA: 16:29 - loss: 0.0767 - acc: 0.97 - ETA: 16:18 - loss: 0.0783 - acc: 0.97 - ETA: 16:08 - loss: 0.0780 - acc: 0.97 - ETA: 15:57 - loss: 0.0773 - acc: 0.97 - ETA: 15:47 - loss: 0.0780 - acc: 0.97 - ETA: 15:39 - loss: 0.0774 - acc: 0.97 - ETA: 15:29 - loss: 0.0774 - acc: 0.97 - ETA: 15:19 - loss: 0.0777 - acc: 0.96 - ETA: 15:11 - loss: 0.0783 - acc: 0.96 - ETA: 15:04 - loss: 0.0787 - acc: 0.96 - ETA: 14:54 - loss: 0.0784 - acc: 0.96 - ETA: 14:43 - loss: 0.0793 - acc: 0.96 - ETA: 14:33 - loss: 0.0797 - acc: 0.96 - ETA: 14:22 - loss: 0.0817 - acc: 0.96 - ETA: 14:11 - loss: 0.0812 - acc: 0.96 - ETA: 14:01 - loss: 0.0834 - acc: 0.96 - ETA: 13:50 - loss: 0.0853 - acc: 0.96 - ETA: 13:40 - loss: 0.0847 - acc: 0.96 - ETA: 13:30 - loss: 0.0850 - acc: 0.96 - ETA: 13:19 - loss: 0.0850 - acc: 0.96 - ETA: 13:08 - loss: 0.0858 - acc: 0.96 - ETA: 12:57 - loss: 0.0873 - acc: 0.96 - ETA: 12:46 - loss: 0.0873 - acc: 0.96 - ETA: 12:36 - loss: 0.0875 - acc: 0.96 - ETA: 12:26 - loss: 0.0884 - acc: 0.96 - ETA: 12:16 - loss: 0.0884 - acc: 0.96 - ETA: 12:06 - loss: 0.0902 - acc: 0.96 - ETA: 11:56 - loss: 0.0913 - acc: 0.96 - ETA: 11:45 - loss: 0.0917 - acc: 0.96 - ETA: 11:34 - loss: 0.0913 - acc: 0.96 - ETA: 11:24 - loss: 0.0910 - acc: 0.96 - ETA: 11:13 - loss: 0.0911 - acc: 0.96 - ETA: 11:03 - loss: 0.0920 - acc: 0.96 - ETA: 10:52 - loss: 0.0915 - acc: 0.96 - ETA: 10:41 - loss: 0.0909 - acc: 0.96 - ETA: 10:30 - loss: 0.0909 - acc: 0.96 - ETA: 10:21 - loss: 0.0918 - acc: 0.96 - ETA: 10:11 - loss: 0.0919 - acc: 0.96 - ETA: 10:02 - loss: 0.0917 - acc: 0.96 - ETA: 9:51 - loss: 0.0917 - acc: 0.9645 - ETA: 9:40 - loss: 0.0922 - acc: 0.964 - ETA: 9:28 - loss: 0.0931 - acc: 0.964 - ETA: 9:17 - loss: 0.0930 - acc: 0.964 - ETA: 9:06 - loss: 0.0924 - acc: 0.964 - ETA: 8:54 - loss: 0.0918 - acc: 0.965 - ETA: 8:43 - loss: 0.0915 - acc: 0.965 - ETA: 8:32 - loss: 0.0909 - acc: 0.965 - ETA: 8:20 - loss: 0.0907 - acc: 0.965 - ETA: 8:10 - loss: 0.0916 - acc: 0.965 - ETA: 7:59 - loss: 0.0915 - acc: 0.965 - ETA: 7:48 - loss: 0.0913 - acc: 0.965 - ETA: 7:37 - loss: 0.0910 - acc: 0.965 - ETA: 7:25 - loss: 0.0908 - acc: 0.965 - ETA: 7:14 - loss: 0.0909 - acc: 0.965 - ETA: 7:02 - loss: 0.0917 - acc: 0.965 - ETA: 6:51 - loss: 0.0913 - acc: 0.965 - ETA: 6:40 - loss: 0.0912 - acc: 0.965 - ETA: 6:28 - loss: 0.0913 - acc: 0.965 - ETA: 6:17 - loss: 0.0937 - acc: 0.965 - ETA: 6:05 - loss: 0.0931 - acc: 0.965 - ETA: 5:54 - loss: 0.0929 - acc: 0.965 - ETA: 5:42 - loss: 0.0935 - acc: 0.965 - ETA: 5:31 - loss: 0.0930 - acc: 0.965 - ETA: 5:19 - loss: 0.0929 - acc: 0.965 - ETA: 5:08 - loss: 0.0927 - acc: 0.965 - ETA: 4:57 - loss: 0.0924 - acc: 0.966 - ETA: 4:45 - loss: 0.0921 - acc: 0.966 - ETA: 4:34 - loss: 0.0924 - acc: 0.966 - ETA: 4:23 - loss: 0.0921 - acc: 0.966 - ETA: 4:11 - loss: 0.0916 - acc: 0.966 - ETA: 4:00 - loss: 0.0911 - acc: 0.967 - ETA: 3:49 - loss: 0.0908 - acc: 0.967 - ETA: 3:37 - loss: 0.0915 - acc: 0.967 - ETA: 3:25 - loss: 0.0911 - acc: 0.967 - ETA: 3:14 - loss: 0.0907 - acc: 0.967 - ETA: 3:03 - loss: 0.0902 - acc: 0.967 - ETA: 2:51 - loss: 0.0905 - acc: 0.967 - ETA: 2:40 - loss: 0.0904 - acc: 0.967 - ETA: 2:28 - loss: 0.0905 - acc: 0.967 - ETA: 2:17 - loss: 0.0901 - acc: 0.967 - ETA: 2:05 - loss: 0.0896 - acc: 0.967 - ETA: 1:54 - loss: 0.0894 - acc: 0.967 - ETA: 1:42 - loss: 0.0893 - acc: 0.967 - ETA: 1:31 - loss: 0.0892 - acc: 0.967 - ETA: 1:19 - loss: 0.0891 - acc: 0.967 - ETA: 1:08 - loss: 0.0898 - acc: 0.967 - ETA: 57s - loss: 0.0894 - acc: 0.967 - ETA: 45s - loss: 0.0889 - acc: 0.96 - ETA: 34s - loss: 0.0888 - acc: 0.96 - ETA: 22s - loss: 0.0884 - acc: 0.96 - ETA: 11s - loss: 0.0880 - acc: 0.96 - 1863s 11s/step - loss: 0.0876 - acc: 0.9680 - val_loss: 0.9800 - val_acc: 0.5625\n",
      "Epoch 2/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163/163 [==============================] - ETA: 28:30 - loss: 0.1202 - acc: 0.93 - ETA: 29:43 - loss: 0.0922 - acc: 0.93 - ETA: 28:56 - loss: 0.1090 - acc: 0.94 - ETA: 28:34 - loss: 0.0831 - acc: 0.96 - ETA: 28:19 - loss: 0.0850 - acc: 0.96 - ETA: 28:23 - loss: 0.0750 - acc: 0.96 - ETA: 28:04 - loss: 0.0983 - acc: 0.96 - ETA: 27:50 - loss: 0.1022 - acc: 0.96 - ETA: 27:32 - loss: 0.0955 - acc: 0.96 - ETA: 27:27 - loss: 0.0928 - acc: 0.96 - ETA: 27:13 - loss: 0.0927 - acc: 0.96 - ETA: 26:58 - loss: 0.0871 - acc: 0.96 - ETA: 27:18 - loss: 0.0821 - acc: 0.96 - ETA: 27:33 - loss: 0.0848 - acc: 0.96 - ETA: 27:18 - loss: 0.0829 - acc: 0.96 - ETA: 27:05 - loss: 0.0801 - acc: 0.96 - ETA: 26:52 - loss: 0.0769 - acc: 0.96 - ETA: 26:39 - loss: 0.0733 - acc: 0.96 - ETA: 26:36 - loss: 0.0724 - acc: 0.97 - ETA: 26:41 - loss: 0.0702 - acc: 0.97 - ETA: 26:34 - loss: 0.0767 - acc: 0.97 - ETA: 26:29 - loss: 0.0737 - acc: 0.97 - ETA: 26:17 - loss: 0.0732 - acc: 0.97 - ETA: 26:08 - loss: 0.0727 - acc: 0.97 - ETA: 26:09 - loss: 0.0709 - acc: 0.97 - ETA: 26:07 - loss: 0.0734 - acc: 0.97 - ETA: 25:59 - loss: 0.0719 - acc: 0.97 - ETA: 25:45 - loss: 0.0713 - acc: 0.97 - ETA: 25:43 - loss: 0.0701 - acc: 0.97 - ETA: 25:30 - loss: 0.0680 - acc: 0.97 - ETA: 25:22 - loss: 0.0695 - acc: 0.97 - ETA: 25:32 - loss: 0.0695 - acc: 0.97 - ETA: 25:15 - loss: 0.0710 - acc: 0.97 - ETA: 25:02 - loss: 0.0728 - acc: 0.97 - ETA: 24:55 - loss: 0.0709 - acc: 0.97 - ETA: 24:45 - loss: 0.0711 - acc: 0.97 - ETA: 24:44 - loss: 0.0695 - acc: 0.97 - ETA: 24:38 - loss: 0.0689 - acc: 0.97 - ETA: 24:29 - loss: 0.0673 - acc: 0.97 - ETA: 24:21 - loss: 0.0689 - acc: 0.97 - ETA: 24:11 - loss: 0.0676 - acc: 0.97 - ETA: 23:56 - loss: 0.0699 - acc: 0.97 - ETA: 23:46 - loss: 0.0735 - acc: 0.97 - ETA: 23:38 - loss: 0.0745 - acc: 0.97 - ETA: 23:29 - loss: 0.0758 - acc: 0.97 - ETA: 23:14 - loss: 0.0743 - acc: 0.97 - ETA: 23:01 - loss: 0.0739 - acc: 0.97 - ETA: 22:47 - loss: 0.0748 - acc: 0.97 - ETA: 22:38 - loss: 0.0739 - acc: 0.97 - ETA: 22:23 - loss: 0.0733 - acc: 0.97 - ETA: 22:09 - loss: 0.0724 - acc: 0.97 - ETA: 21:58 - loss: 0.0749 - acc: 0.97 - ETA: 21:48 - loss: 0.0738 - acc: 0.97 - ETA: 21:46 - loss: 0.0746 - acc: 0.97 - ETA: 21:44 - loss: 0.0766 - acc: 0.97 - ETA: 21:42 - loss: 0.0759 - acc: 0.97 - ETA: 21:36 - loss: 0.0763 - acc: 0.97 - ETA: 21:39 - loss: 0.0762 - acc: 0.97 - ETA: 21:36 - loss: 0.0756 - acc: 0.97 - ETA: 21:23 - loss: 0.0750 - acc: 0.97 - ETA: 21:16 - loss: 0.0746 - acc: 0.97 - ETA: 21:09 - loss: 0.0736 - acc: 0.97 - ETA: 21:07 - loss: 0.0757 - acc: 0.97 - ETA: 20:55 - loss: 0.0761 - acc: 0.97 - ETA: 20:40 - loss: 0.0759 - acc: 0.97 - ETA: 20:26 - loss: 0.0756 - acc: 0.97 - ETA: 20:13 - loss: 0.0757 - acc: 0.97 - ETA: 19:57 - loss: 0.0751 - acc: 0.97 - ETA: 19:42 - loss: 0.0755 - acc: 0.97 - ETA: 19:29 - loss: 0.0746 - acc: 0.97 - ETA: 19:15 - loss: 0.0737 - acc: 0.97 - ETA: 19:00 - loss: 0.0731 - acc: 0.97 - ETA: 18:46 - loss: 0.0734 - acc: 0.97 - ETA: 18:31 - loss: 0.0727 - acc: 0.97 - ETA: 18:17 - loss: 0.0722 - acc: 0.97 - ETA: 18:02 - loss: 0.0724 - acc: 0.97 - ETA: 17:48 - loss: 0.0716 - acc: 0.97 - ETA: 17:38 - loss: 0.0709 - acc: 0.97 - ETA: 17:26 - loss: 0.0705 - acc: 0.97 - ETA: 17:12 - loss: 0.0704 - acc: 0.97 - ETA: 16:58 - loss: 0.0696 - acc: 0.97 - ETA: 16:44 - loss: 0.0692 - acc: 0.97 - ETA: 16:30 - loss: 0.0689 - acc: 0.97 - ETA: 16:16 - loss: 0.0685 - acc: 0.97 - ETA: 16:02 - loss: 0.0679 - acc: 0.97 - ETA: 15:48 - loss: 0.0675 - acc: 0.97 - ETA: 15:38 - loss: 0.0682 - acc: 0.97 - ETA: 15:28 - loss: 0.0685 - acc: 0.97 - ETA: 15:18 - loss: 0.0685 - acc: 0.97 - ETA: 15:04 - loss: 0.0679 - acc: 0.97 - ETA: 14:50 - loss: 0.0676 - acc: 0.97 - ETA: 14:37 - loss: 0.0674 - acc: 0.97 - ETA: 14:23 - loss: 0.0678 - acc: 0.97 - ETA: 14:09 - loss: 0.0671 - acc: 0.97 - ETA: 13:56 - loss: 0.0678 - acc: 0.97 - ETA: 13:44 - loss: 0.0674 - acc: 0.97 - ETA: 13:33 - loss: 0.0671 - acc: 0.97 - ETA: 13:22 - loss: 0.0665 - acc: 0.97 - ETA: 13:09 - loss: 0.0676 - acc: 0.97 - ETA: 12:56 - loss: 0.0676 - acc: 0.97 - ETA: 12:43 - loss: 0.0690 - acc: 0.97 - ETA: 12:30 - loss: 0.0684 - acc: 0.97 - ETA: 12:17 - loss: 0.0683 - acc: 0.97 - ETA: 12:04 - loss: 0.0696 - acc: 0.97 - ETA: 11:51 - loss: 0.0690 - acc: 0.97 - ETA: 11:39 - loss: 0.0689 - acc: 0.97 - ETA: 11:28 - loss: 0.0684 - acc: 0.97 - ETA: 11:16 - loss: 0.0679 - acc: 0.97 - ETA: 11:03 - loss: 0.0674 - acc: 0.97 - ETA: 10:50 - loss: 0.0673 - acc: 0.97 - ETA: 10:38 - loss: 0.0675 - acc: 0.97 - ETA: 10:26 - loss: 0.0674 - acc: 0.97 - ETA: 10:14 - loss: 0.0674 - acc: 0.97 - ETA: 10:01 - loss: 0.0669 - acc: 0.97 - ETA: 9:48 - loss: 0.0672 - acc: 0.9747 - ETA: 9:36 - loss: 0.0668 - acc: 0.974 - ETA: 9:23 - loss: 0.0693 - acc: 0.974 - ETA: 9:11 - loss: 0.0699 - acc: 0.974 - ETA: 8:59 - loss: 0.0712 - acc: 0.974 - ETA: 8:47 - loss: 0.0708 - acc: 0.974 - ETA: 8:35 - loss: 0.0722 - acc: 0.973 - ETA: 8:22 - loss: 0.0718 - acc: 0.973 - ETA: 8:09 - loss: 0.0715 - acc: 0.973 - ETA: 7:56 - loss: 0.0714 - acc: 0.973 - ETA: 7:44 - loss: 0.0710 - acc: 0.974 - ETA: 7:31 - loss: 0.0705 - acc: 0.974 - ETA: 7:19 - loss: 0.0701 - acc: 0.974 - ETA: 7:06 - loss: 0.0697 - acc: 0.974 - ETA: 6:54 - loss: 0.0701 - acc: 0.974 - ETA: 6:42 - loss: 0.0697 - acc: 0.974 - ETA: 6:29 - loss: 0.0698 - acc: 0.974 - ETA: 6:17 - loss: 0.0714 - acc: 0.974 - ETA: 6:05 - loss: 0.0712 - acc: 0.974 - ETA: 5:52 - loss: 0.0709 - acc: 0.974 - ETA: 5:40 - loss: 0.0708 - acc: 0.974 - ETA: 5:28 - loss: 0.0708 - acc: 0.974 - ETA: 5:15 - loss: 0.0704 - acc: 0.974 - ETA: 5:03 - loss: 0.0700 - acc: 0.975 - ETA: 4:51 - loss: 0.0697 - acc: 0.975 - ETA: 4:38 - loss: 0.0698 - acc: 0.975 - ETA: 4:26 - loss: 0.0696 - acc: 0.975 - ETA: 4:14 - loss: 0.0697 - acc: 0.975 - ETA: 4:01 - loss: 0.0704 - acc: 0.975 - ETA: 3:49 - loss: 0.0702 - acc: 0.975 - ETA: 3:37 - loss: 0.0711 - acc: 0.975 - ETA: 3:25 - loss: 0.0719 - acc: 0.975 - ETA: 3:13 - loss: 0.0722 - acc: 0.975 - ETA: 3:00 - loss: 0.0724 - acc: 0.974 - ETA: 2:48 - loss: 0.0727 - acc: 0.974 - ETA: 2:36 - loss: 0.0725 - acc: 0.975 - ETA: 2:24 - loss: 0.0721 - acc: 0.975 - ETA: 2:12 - loss: 0.0726 - acc: 0.974 - ETA: 2:00 - loss: 0.0724 - acc: 0.975 - ETA: 1:47 - loss: 0.0721 - acc: 0.975 - ETA: 1:35 - loss: 0.0723 - acc: 0.975 - ETA: 1:23 - loss: 0.0722 - acc: 0.975 - ETA: 1:11 - loss: 0.0718 - acc: 0.975 - ETA: 59s - loss: 0.0718 - acc: 0.975 - ETA: 47s - loss: 0.0715 - acc: 0.97 - ETA: 35s - loss: 0.0716 - acc: 0.97 - ETA: 23s - loss: 0.0713 - acc: 0.97 - ETA: 11s - loss: 0.0710 - acc: 0.97 - 1946s 12s/step - loss: 0.0713 - acc: 0.9757 - val_loss: 0.5191 - val_acc: 0.8125\n",
      "Epoch 3/15\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 60/163 [==========>...................] - ETA: 27:17 - loss: 0.0367 - acc: 1.00 - ETA: 27:16 - loss: 0.0450 - acc: 0.98 - ETA: 27:15 - loss: 0.0437 - acc: 0.98 - ETA: 27:01 - loss: 0.0780 - acc: 0.98 - ETA: 26:59 - loss: 0.0773 - acc: 0.97 - ETA: 26:53 - loss: 0.0997 - acc: 0.96 - ETA: 26:38 - loss: 0.0908 - acc: 0.96 - ETA: 26:29 - loss: 0.0879 - acc: 0.96 - ETA: 26:21 - loss: 0.0815 - acc: 0.96 - ETA: 26:08 - loss: 0.0829 - acc: 0.96 - ETA: 26:00 - loss: 0.0796 - acc: 0.96 - ETA: 25:50 - loss: 0.0939 - acc: 0.96 - ETA: 25:39 - loss: 0.1014 - acc: 0.95 - ETA: 25:31 - loss: 0.1089 - acc: 0.95 - ETA: 25:19 - loss: 0.1037 - acc: 0.96 - ETA: 25:09 - loss: 0.0995 - acc: 0.96 - ETA: 25:00 - loss: 0.0958 - acc: 0.96 - ETA: 24:48 - loss: 0.0939 - acc: 0.96 - ETA: 24:38 - loss: 0.0902 - acc: 0.96 - ETA: 24:29 - loss: 0.0874 - acc: 0.96 - ETA: 24:18 - loss: 0.0946 - acc: 0.96 - ETA: 24:08 - loss: 0.0949 - acc: 0.96 - ETA: 23:59 - loss: 0.0916 - acc: 0.96 - ETA: 23:47 - loss: 0.0891 - acc: 0.96 - ETA: 23:37 - loss: 0.0867 - acc: 0.96 - ETA: 23:27 - loss: 0.0854 - acc: 0.96 - ETA: 23:16 - loss: 0.0833 - acc: 0.96 - ETA: 23:07 - loss: 0.0885 - acc: 0.96 - ETA: 22:56 - loss: 0.0857 - acc: 0.96 - ETA: 22:46 - loss: 0.0838 - acc: 0.96 - ETA: 22:36 - loss: 0.0816 - acc: 0.97 - ETA: 22:24 - loss: 0.0817 - acc: 0.97 - ETA: 22:14 - loss: 0.0796 - acc: 0.97 - ETA: 22:04 - loss: 0.0779 - acc: 0.97 - ETA: 21:53 - loss: 0.0767 - acc: 0.97 - ETA: 21:43 - loss: 0.0773 - acc: 0.97 - ETA: 21:33 - loss: 0.0806 - acc: 0.97 - ETA: 21:22 - loss: 0.0786 - acc: 0.97 - ETA: 21:12 - loss: 0.0789 - acc: 0.97 - ETA: 21:02 - loss: 0.0790 - acc: 0.97 - ETA: 20:51 - loss: 0.0791 - acc: 0.97 - ETA: 20:42 - loss: 0.0783 - acc: 0.97 - ETA: 20:31 - loss: 0.0771 - acc: 0.97 - ETA: 20:21 - loss: 0.0765 - acc: 0.97 - ETA: 20:11 - loss: 0.0799 - acc: 0.96 - ETA: 20:00 - loss: 0.0786 - acc: 0.97 - ETA: 19:49 - loss: 0.0774 - acc: 0.97 - ETA: 19:39 - loss: 0.0760 - acc: 0.97 - ETA: 19:29 - loss: 0.0764 - acc: 0.97 - ETA: 19:19 - loss: 0.0784 - acc: 0.97 - ETA: 19:08 - loss: 0.0770 - acc: 0.97 - ETA: 18:58 - loss: 0.0760 - acc: 0.97 - ETA: 18:48 - loss: 0.0748 - acc: 0.97 - ETA: 18:37 - loss: 0.0736 - acc: 0.97 - ETA: 18:27 - loss: 0.0734 - acc: 0.97 - ETA: 18:18 - loss: 0.0738 - acc: 0.97 - ETA: 18:13 - loss: 0.0725 - acc: 0.97 - ETA: 18:02 - loss: 0.0726 - acc: 0.97 - ETA: 17:53 - loss: 0.0716 - acc: 0.97 - ETA: 17:45 - loss: 0.0716 - acc: 0.9734"
     ]
    }
   ],
   "source": [
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    "    validation_data=validation_generator,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 56s - ETA: 42 - ETA: 28 - ETA: 14 - 277s 14s/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict_generator(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_accuracy = keras.metrics.categorical_accuracy(test_generator.classes, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20/20 [==============================] - ETA: 5: - ETA: 4: - ETA: 4: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 3: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 53s - ETA: 40 - ETA: 26 - ETA: 13 - 264s 13s/step\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate_generator(generator=test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 68.91%\n",
      "Loss: 1.19\n"
     ]
    }
   ],
   "source": [
    "print(\"%s%.2f%s\"% (\"Accuracy: \", result[1]*100, \"%\"))\n",
    "print(\"%s%.2f\"% (\"Loss: \", result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_values(gen):\n",
    "    vals = []\n",
    "    for i in gen:\n",
    "        vals.append(gen[i])\n",
    "    return vals\n",
    "\n",
    "def get_classes(gen):\n",
    "    rev_gen = {}\n",
    "    for i in gen:\n",
    "        rev_gen[gen[i]] = i\n",
    "    return rev_gen\n",
    "\n",
    "def find_max(preds_i):\n",
    "    num = -1\n",
    "    index = -1\n",
    "    for i in range(len(preds_i)):\n",
    "        if preds_i[i]>num:\n",
    "            num = preds_i[i]\n",
    "            index = i\n",
    "    return index  \n",
    "\n",
    "def get_label(cls, val):\n",
    "    return cls[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = get_class_values(test_generator.class_indices)\n",
    "cls = get_classes(test_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "acc = 0\n",
    "direc = \"data\\\\pre\\\\test\"\n",
    "files = test_generator.filenames\n",
    "for i in range(len(preds)):\n",
    "    cls_index = find_max(preds[i])\n",
    "    pred_label = cls[cls_index]\n",
    "    org_label = cls[test_generator.classes[i]]\n",
    "    file_name = os.path.join(direc, files[i])\n",
    "    if org_label==pred_label:\n",
    "        acc +=1\n",
    "    if total%10==0:\n",
    "        result = \"Wrong!!!\\n\"\n",
    "        if org_label==pred_label:\n",
    "            result = \"Right!!!\\n\"\n",
    "        title_text = \"Original: \"+ org_label+\"\\nPredicted: \"+ pred_label+\"\\nFile: \"+ file_name\n",
    "        title_text=result+ title_text\n",
    "        \n",
    "        img = cv2.imread(file_name, 3)\n",
    "        if org_label==pred_label:\n",
    "            plt.title(title_text, color='blue')\n",
    "        else:\n",
    "            plt.title(title_text, color='red')\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "    \n",
    "    total+=1\n",
    "accuracy = (acc/total)*100\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
