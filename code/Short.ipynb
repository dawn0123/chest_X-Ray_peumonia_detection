{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andromeda\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "import shutil\n",
    "import inspect\n",
    "\n",
    "import gc\n",
    "\n",
    "import re\n",
    "\n",
    "import keras\n",
    "from keras import models\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling2D, GlobalAveragePooling1D, Flatten\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates directory, if directory exists removes if remove parameter is set to True \n",
    "def create_directory(directory_path, remove=False):\n",
    "    if remove and os.path.exists(directory_path):\n",
    "        try:\n",
    "            shutil.rmtree(directory_path)\n",
    "            os.mkdir(directory_path)\n",
    "        except:\n",
    "            print(\"Could not remove directory : \", directory_path)\n",
    "            return False\n",
    "    else:\n",
    "        try:\n",
    "            os.mkdir(directory_path)\n",
    "        except:\n",
    "            print(\"Could not create directory: \", directory_path)\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "# Removes directory, if directory exists \n",
    "def remove_directory(directory_path):\n",
    "    if os.path.exists(directory_path):\n",
    "        try:\n",
    "            shutil.rmtree(directory_path)\n",
    "        except:\n",
    "            print(\"Could not remove directory : \", directory_path)\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print date and time for given type of representation\n",
    "def date_time(x):\n",
    "    if x==1:\n",
    "        print('Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "    if x==2:    \n",
    "        print('Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "    if x==3:  \n",
    "        print('Date now: %s' % datetime.datetime.now())\n",
    "    if x==4:  \n",
    "        print('Date today: %s' % datetime.date.today())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints a integer for degugging\n",
    "def debug(x):\n",
    "    print(\"-\"*40, x, \"-\"*40)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes everything except alphabetical and selected characters from name string\n",
    "def name_correct(name):\n",
    "    return re.sub(r'[^a-zA-Z,:]', ' ', name).title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset tensorflow graph tp free up memory and resource allocation \n",
    "def reset_graph(model=None):\n",
    "    try:\n",
    "        del model\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "# reset callbacks \n",
    "def reset_callbacks(checkpoint=None, reduce_lr=None, early_stopping=None, tensorboard=None):\n",
    "    checkpoint=None\n",
    "    reduce_lr = None\n",
    "    early_stopping = None\n",
    "    tensorboard = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-24a8d2cd135d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreset_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mreset_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "reset_graph(model)\n",
    "reset_callbacks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['NORMAL', 'PNEUMONIA']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure input/ output directory\n",
    "# Configure training, validation, testing directory\n",
    "\n",
    "input_directory = r\"data/input/\"\n",
    "output_directory = r\"data/output/\"\n",
    "\n",
    "training_dir = input_directory+ r\"train\"\n",
    "validation_dir = input_directory+ r\"val\"\n",
    "testing_dir = input_directory+ r\"test\"\n",
    "\n",
    "classes = os.listdir(training_dir)\n",
    "classes = ['NORMAL', 'PNEUMONIA']\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 320 images belonging to 2 classes.\n",
      "Found 320 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "target_size = (299,299)\n",
    "# target_size = (150,150)\n",
    "\n",
    "rescale=1./255.0\n",
    "# batch_size = 32\n",
    "\n",
    "batch_size = 128\n",
    "\n",
    "# class_mode = \"binary\"\n",
    "\n",
    "class_mode='categorical'\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    fill_mode='nearest',\n",
    "    rescale=rescale)     \n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=target_size,\n",
    "    classes = classes,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,\n",
    "    classes = classes,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size)    \n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    testing_dir,\n",
    "    target_size=target_size,\n",
    "    classes = classes,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.9448173 , 0.67303226])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight\n",
    "\n",
    "class_weight = class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes)\n",
    "class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "# Model Directory\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "model_dir=output_directory + r\"models/\"+time.strftime('%Y-%m-%d %H-%M-%S')+\"/\"\n",
    "# model_dir=output_directory + r\"models/\"+time.strftime('%Y%m%d%H%M%S')+\"/\"\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "# Log Directory\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "# log_dir=output_directory + r\"logs/\"+time.strftime('%Y%m%d%H%M%S')\n",
    "log_dir=output_directory + r\"logs/\"+time.strftime('%Y-%m-%d %H-%M-%S')\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# Create Output Directory (Model and Log)\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "create_directory(model_dir, remove=True)\n",
    "create_directory(log_dir, remove=True)\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "# Model File Name Configuration\n",
    "#------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "# Initial Trainning Model Filename \n",
    "#----------------------------------------------------------------------------------------------------#\n",
    "init_model_file=model_dir+\"base-\"+\"{epoch:02d}-val_acc-{val_acc:.2f}-val_loss-{val_loss:.2f}.hdf5\"\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "# Trainning Model Filename \n",
    "#----------------------------------------------------------------------------------------------------#\n",
    "model_file=model_dir+\"{epoch:02d}-val_acc-{val_acc:.2f}-val_loss-{val_loss:.2f}.hdf5\"\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "# Retrainning Model Filename \n",
    "#----------------------------------------------------------------------------------------------------#\n",
    "retrain_model_file=model_dir+\"retrain-{epoch:02d}-val_acc-{val_acc:.2f}-val_loss-{val_loss:.2f}.hdf5\"\n",
    "#--------------------------------------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-20-455a2dc4cf83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreset_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "reset_graph(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def model():\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "    # x = base_model.output\n",
    "    # x = GlobalAveragePooling2D()(x)\n",
    "    # x = Dense(1024, activation='relu')(x)\n",
    "    # predictions = Dense(2, activation='softmax')(x) \n",
    "\n",
    "\n",
    "    x = base_model.output\n",
    "    x = Dropout(0.5)(x)\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "#     x = Dense(1024, activation='relu')(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    # x = Dense(128, activation='relu')(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # x = Flatten()(x)\n",
    "    predictions = Dense(2, activation='softmax')(x) \n",
    "\n",
    "\n",
    "    # x = base_model.output\n",
    "    # x = Flatten()(x)\n",
    "    # predictions = Dense(2, activation='softmax')(x)\n",
    "\n",
    "    # x = base_model.output\n",
    "    # x = Dropout(0.5)(x)\n",
    "    # x = GlobalAveragePooling2D()(x)\n",
    "    # x = Dense(128, activation='relu')(x)\n",
    "    # x = BatchNormalization()(x)\n",
    "    # predictions = Dense(2, activation='sigmoid')(x)\n",
    "\n",
    "\n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "\n",
    "    # # if print_layers:\n",
    "    # #     for i, layer in enumerate(base_model.layers):\n",
    "    # #         print(i, layer.name)\n",
    "\n",
    "\n",
    "    # for layer in model.layers[:249]:\n",
    "    #     layer.trainable = False\n",
    "    # for layer in model.layers[249:]:\n",
    "    #     layer.trainable = True\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    model_file, \n",
    "    monitor='val_loss',\n",
    "    verbose=0,\n",
    "    save_best_only=False, \n",
    "    save_weights_only=False, \n",
    "    mode='auto',\n",
    "    period=1)\n",
    "\n",
    "# early_stopping = EarlyStopping(\n",
    "#     monitor='val_loss',\n",
    "#     min_delta=0, \n",
    "#     patience=0, \n",
    "#     verbose=0, \n",
    "#     mode='auto', \n",
    "#     baseline=None, \n",
    "#     restore_best_weights=False)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_acc', \n",
    "    patience=3, \n",
    "    verbose=1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    histogram_freq=0, \n",
    "    batch_size=batch_size, \n",
    "    write_graph=True, \n",
    "    write_grads=False, \n",
    "    write_images=False, \n",
    "    embeddings_freq=0,\n",
    "    embeddings_layer_names=None, \n",
    "    embeddings_metadata=None, \n",
    "    embeddings_data=None, \n",
    "    update_freq='epoch')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# reduce_lr = ReduceLROnPlateau(\n",
    "#     monitor='val_loss',\n",
    "#     factor=0.1,\n",
    "#     patience=2,\n",
    "#     cooldown=2,\n",
    "#     min_lr=0.00001,\n",
    "#     verbose=1)\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_acc',\n",
    "    patience=2,\n",
    "    cooldown=2,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "from keras.callbacks import Callback\n",
    "    \n",
    "    \n",
    "from keras import backend as K\n",
    "\n",
    "def check_units(y_true, y_pred):\n",
    "    if y_pred.shape[1] != 1:\n",
    "        y_pred = y_pred[:,1:2]\n",
    "        y_true = y_true[:,1:2]\n",
    "    return y_true, y_pred\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    y_true, y_pred = check_units(y_true, y_pred)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    y_true, y_pred = check_units(y_true, y_pred)\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def f1(y_true, y_pred):\n",
    "    def recall(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "        recall = true_positives / (possible_positives + K.epsilon())\n",
    "        return recall\n",
    "\n",
    "    def precision(y_true, y_pred):\n",
    "        true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "        predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "        precision = true_positives / (predicted_positives + K.epsilon())\n",
    "        return precision\n",
    "    y_true, y_pred = check_units(y_true, y_pred)\n",
    "    precision = precision(y_true, y_pred)\n",
    "    recall = recall(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "\n",
    "def macro_f1(y_true, y_pred):\n",
    "    y_pred = K.round(y_pred)\n",
    "    tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "    # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "    fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "    fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "    p = tp / (tp + fp + K.epsilon())\n",
    "    r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "    f1 = 2*p*r / (p+r+K.epsilon())\n",
    "    f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "    return K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 2018-12-08 19:50:08\n"
     ]
    }
   ],
   "source": [
    "date_time(1)\n",
    "\n",
    "# callbacks = [checkpoint, reduce_lr, early_stopping, tensorboard]\n",
    "callbacks = [checkpoint, reduce_lr, early_stopping, tensorboard]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time(1)\n",
    "model = model()\n",
    "date_time(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optimizers.Adam(lr=0.00001)\n",
    "loss='categorical_crossentropy'\n",
    "# metrics=['accuracy']\n",
    "\n",
    "\n",
    "# model.compile(optimizer, loss=loss, metrics=metrics)\n",
    "model.compile(loss=loss,\n",
    "              optimizer= optimizer,\n",
    "              metrics=['accuracy', precision, recall, f1])\n",
    "\n",
    "date_time(1)\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = len(train_generator),\n",
    "    epochs=30,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    class_weight=class_weight)\n",
    "\n",
    "\n",
    "date_time(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 2018-12-08 23:21:17\n",
      "Epoch 1/30\n",
      " 1/41 [..............................] - ETA: 9:20 - loss: 0.0248 - acc: 0.9844 - precision: 0.9787 - recall: 1.0000 - f1: 0.9892"
     ]
    }
   ],
   "source": [
    "date_time(1)\n",
    "\n",
    "cur_model_file = r\"data\\output\\models\\2018-12-08 17-16-53\\03-val_acc-0.94-val_loss-0.16.hdf5\"\n",
    "\n",
    "cur_model = keras.models.load_model(cur_model_file, custom_objects={'accuracy':'accuracy', 'precision':precision, 'recall':recall, 'f1':f1})\n",
    "\n",
    "# cur_model = keras.models.load_model(cur_model_file)\n",
    "optimizer=optimizers.Adam(lr=0.00001)\n",
    "loss='categorical_crossentropy'\n",
    "\n",
    "# metrics=['accuracy']\n",
    "# model.compile(optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "cur_model.compile(\n",
    "    loss=loss,\n",
    "    optimizer= optimizer,\n",
    "    metrics=['accuracy', precision, recall, f1])\n",
    "\n",
    "\n",
    "history = cur_model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = len(train_generator),\n",
    "    epochs=30,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    class_weight=class_weight)\n",
    "\n",
    "\n",
    "date_time(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 2018-12-07 16-00-01\n",
      "1 2018-12-07 17-02-04\n",
      "2 2018-12-08 04-35-31\n",
      "3 2018-12-08 17-16-53\n"
     ]
    }
   ],
   "source": [
    "model_dir = \"data/output/models/\"\n",
    "\n",
    "dirs = os.listdir(model_dir)\n",
    "for i in range(len(dirs)):\n",
    "    print(i, dirs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/output/models/2018-12-08 17-16-53/\n",
      "0 01-val_acc-0.78-val_loss-0.44.hdf5\n",
      "1 01-val_acc-0.92-val_loss-0.21.hdf5\n",
      "2 02-val_acc-0.77-val_loss-0.55.hdf5\n",
      "3 02-val_acc-0.92-val_loss-0.24.hdf5\n",
      "4 03-val_acc-0.78-val_loss-0.50.hdf5\n",
      "5 03-val_acc-0.94-val_loss-0.16.hdf5\n",
      "6 04-val_acc-0.77-val_loss-0.61.hdf5\n",
      "7 04-val_acc-0.91-val_loss-0.21.hdf5\n",
      "8 05-val_acc-0.93-val_loss-0.20.hdf5\n",
      "9 06-val_acc-0.92-val_loss-0.21.hdf5\n"
     ]
    }
   ],
   "source": [
    "current_dir = model_dir+dirs[3]+\"/\"\n",
    "\n",
    "print(current_dir)\n",
    "cur_models = os.listdir(current_dir)\n",
    "for i in range(len(cur_models)):\n",
    "    print(i, cur_models[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/output/models/2018-12-08 17-16-53/03-val_acc-0.94-val_loss-0.16.hdf5\n"
     ]
    }
   ],
   "source": [
    "cur_model = cur_models[5]\n",
    "model_file = current_dir+cur_model\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(model_file, custom_objects={'accuracy':'accuracy', 'precision':precision, 'recall':recall, 'f1':f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timestamp: 2018-12-08 19:37:28\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": "OOM when allocating tensor with shape[128,64,147,147] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d_97_5/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@batch_normalization_97_5/cond/FusedBatchNorm/Switch\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](activation_96_5/Relu, conv2d_97_5/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss_7/mul/_24563}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3322_loss_7/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-79-628caae2b7e2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mdate_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# result = model.evaluate_generator(generator=test_generator, steps=100, verbose=1)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate_generator\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msteps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_generator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\legacy\\interfaces.py\u001b[0m in \u001b[0;36mwrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     89\u001b[0m                 warnings.warn('Update your `' + object_name + '` call to the ' +\n\u001b[0;32m     90\u001b[0m                               'Keras 2 API: ' + signature, stacklevel=2)\n\u001b[1;32m---> 91\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     92\u001b[0m         \u001b[0mwrapper\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_function\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(self, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m   1470\u001b[0m             \u001b[0mworkers\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m             \u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_multiprocessing\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m             verbose=verbose)\n\u001b[0m\u001b[0;32m   1473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0minterfaces\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_generator_methods_support\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_generator.py\u001b[0m in \u001b[0;36mevaluate_generator\u001b[1;34m(model, generator, steps, max_queue_size, workers, use_multiprocessing, verbose)\u001b[0m\n\u001b[0;32m    344\u001b[0m                                  \u001b[1;34m'or (x, y). Found: '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    345\u001b[0m                                  str(generator_output))\n\u001b[1;32m--> 346\u001b[1;33m             \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_on_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    347\u001b[0m             \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    348\u001b[0m             \u001b[0mouts_per_batch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mtest_on_batch\u001b[1;34m(self, x, y, sample_weight)\u001b[0m\n\u001b[0;32m   1254\u001b[0m             \u001b[0mins\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0msample_weights\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1255\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_make_test_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1256\u001b[1;33m         \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtest_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1257\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0munpack_singleton\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1258\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2713\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2714\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2715\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2716\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2717\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[1;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   2673\u001b[0m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_metadata\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2674\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2675\u001b[1;33m             \u001b[0mfetched\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2676\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2677\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\client\\session.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1437\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[0;32m   1438\u001b[0m               \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1439\u001b[1;33m               run_metadata_ptr)\n\u001b[0m\u001b[0;32m   1440\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1441\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\framework\\errors_impl.py\u001b[0m in \u001b[0;36m__exit__\u001b[1;34m(self, type_arg, value_arg, traceback_arg)\u001b[0m\n\u001b[0;32m    526\u001b[0m             \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    527\u001b[0m             \u001b[0mcompat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_api\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTF_Message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstatus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 528\u001b[1;33m             c_api.TF_GetCode(self.status.status))\n\u001b[0m\u001b[0;32m    529\u001b[0m     \u001b[1;31m# Delete the underlying status object from memory otherwise it stays alive\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    530\u001b[0m     \u001b[1;31m# as there is a reference to status from this from the traceback due to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m: OOM when allocating tensor with shape[128,64,147,147] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[{{node conv2d_97_5/convolution}} = Conv2D[T=DT_FLOAT, _class=[\"loc:@batch_normalization_97_5/cond/FusedBatchNorm/Switch\"], data_format=\"NCHW\", dilations=[1, 1, 1, 1], padding=\"SAME\", strides=[1, 1, 1, 1], use_cudnn_on_gpu=true, _device=\"/job:localhost/replica:0/task:0/device:GPU:0\"](activation_96_5/Relu, conv2d_97_5/kernel/read)]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n\n\t [[{{node loss_7/mul/_24563}} = _Recv[client_terminated=false, recv_device=\"/job:localhost/replica:0/task:0/device:CPU:0\", send_device=\"/job:localhost/replica:0/task:0/device:GPU:0\", send_device_incarnation=1, tensor_name=\"edge_3322_loss_7/mul\", tensor_type=DT_FLOAT, _device=\"/job:localhost/replica:0/task:0/device:CPU:0\"]()]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n"
     ]
    }
   ],
   "source": [
    "date_time(1)\n",
    "# result = model.evaluate_generator(generator=test_generator, steps=100, verbose=1)\n",
    "result = model.evaluate_generator(generator=test_generator, steps=len(test_generator), verbose=1)\n",
    "        \n",
    "    \n",
    "accuracy = result[1]*100\n",
    "loss = result[0]\n",
    "\n",
    "print(\"%s%.2f%s\"% (\"Accuracy: \", accuracy, \"%\"))\n",
    "print(\"%s%.2f\"% (\"Loss: \", loss))\n",
    "\n",
    "date_time(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time(1)\n",
    "\n",
    "# y_preds = model.predict_generator(test_generator, steps=len(test_generator), verbose=1)\n",
    "y_preds = model.predict_generator(validation_generator, steps=len(validation_generator), verbose=1)\n",
    "        \n",
    "y_classes = y_preds.argmax(axis=-1)\n",
    "date_time(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = confusion_matrix(test_generator.classes, y_classes)\n",
    "\n",
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=CM ,  \n",
    "    figsize=(10,8), \n",
    "    hide_ticks=True,cmap=plt.cm.Blues)\n",
    "# plt.title(\"Model File: \"+cur_model)\n",
    "plt.xticks(range(len(classes)), classes, fontsize=14)\n",
    "plt.yticks(range(len(classes)), classes, fontsize=14)\n",
    "# cm_name = \"data/output/figures/CM-\"+cur_model[:-5]+\".png\"\n",
    "# print(cm_name)\n",
    "# plt.savefig(\"data/output/figures/CM\"+cur_model+\".png\", dpi=None, facecolor='w', edgecolor='w',\n",
    "#         orientation='portrait', papertype=None, format=None,\n",
    "#         transparent=False, bbox_inches=None, pad_inches=0.1,\n",
    "#         frameon=None, metadata=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = confusion_matrix(test_generator.classes, y_classes)\n",
    "\n",
    "cls_report_print = classification_report(test_generator.classes, y_classes, target_names=classes)\n",
    "\n",
    "# cls_report = classification_report(test_generator.classes, y_classes, target_names=classes, output_dict=True)\n",
    "\n",
    "# if print_report: \n",
    "print(cls_report_print)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4883508451347647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andromeda\\Anaconda3\\lib\\site-packages\\sklearn\\metrics\\ranking.py:656: UndefinedMetricWarning: No positive samples in y_true, true positive value should be meaningless\n",
      "  UndefinedMetricWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "y_true=test_generator.classes\n",
    "y_pred = y_preds.argmax(-1) \n",
    "\n",
    "x=metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "x=metrics.accuracy_score(y_true, y_pred, normalize=False)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred, pos_label=2)\n",
    "\n",
    "x=metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "x=metrics.average_precision_score(y_true, y_pred) \n",
    "\n",
    "\n",
    "x=metrics.balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "x=metrics.brier_score_loss(y_true, 1-y_pred, pos_label=1) \n",
    "\n",
    "x=metrics.f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "x=metrics.f1_score(y_true, y_pred, average='macro')\n",
    "x=metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "x=metrics.f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "\n",
    "\n",
    "# x=metrics.fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n",
    "\n",
    "# x=metrics.fbeta_score(y_true, y_pred, average='micro', beta=0.5)\n",
    "\n",
    "# x=metrics.fbeta_score(y_true, y_pred, average='weighted', beta=0.5)\n",
    "\n",
    "# x=metrics.fbeta_score(y_true, y_pred, average=None, beta=0.5)\n",
    "\n",
    "# x=metrics.hamming_loss(y_true, y_pred)\n",
    "\n",
    "x=metrics.roc_auc_score(y_true, y_pred)\n",
    "print(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
