{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Andromeda\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from os import listdir, makedirs\n",
    "from os.path import join, exists, expanduser\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import datetime as dt\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "import cv2\n",
    "\n",
    "import keras\n",
    "\n",
    "from keras.callbacks import TensorBoard\n",
    "\n",
    "from keras.preprocessing import image\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.models import Model\n",
    "\n",
    "\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\n",
    "from keras.layers import Dense, GlobalAveragePooling2D\n",
    "from keras import optimizers\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "# from keras.applications import vgg16, resnet50, mobilenet\n",
    "# from keras.applications.vgg16 import VGG16\n",
    "# from keras.applications.resnet50 import ResNet50\n",
    "# from keras.applications import xception\n",
    "# from keras.applications import inception_v3\n",
    "\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "\n",
    "from keras.applications.vgg16 import preprocess_input, decode_predictions\n",
    "\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "np.random.seed(2)\n",
    "sns.set(style='white', context='notebook', palette='deep')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "# move file in accordance to percent\n",
    "def move(from_path, to_path, percent):\n",
    "    from_small = False\n",
    "    from_files = os.listdir(from_path)\n",
    "    to_files = os.listdir(to_path)\n",
    "    \n",
    "    from_files_len = len(from_files)\n",
    "    to_files_len = len(to_files)\n",
    "    \n",
    "    total_files_num = from_files_len + to_files_len\n",
    "    \n",
    "    max_num = (total_files_num)*(percent/100)\n",
    "    max_num = max_num-1\n",
    "    \n",
    "    if from_files_len<to_files_len:\n",
    "        from_path, to_path = to_path, from_path\n",
    "        from_files = os.listdir(from_path)\n",
    "        to_files = os.listdir(to_path)\n",
    "        \n",
    "    \n",
    "    i=len(to_files)-1\n",
    "    \n",
    "    for name in tqdm(from_files):\n",
    "        from_small = True\n",
    "        if i>=max_num:\n",
    "            break\n",
    "            \n",
    "        from_file = os.path.join(from_path, name)\n",
    "        to_file = os.path.join(to_path, name)\n",
    "        shutil.move(from_file, to_file)\n",
    "        i+=1\n",
    "\n",
    "    if from_small:\n",
    "        from_path, to_path = to_path, from_path\n",
    "    \n",
    "    from_files = os.listdir(from_path)\n",
    "    to_files = os.listdir(to_path)\n",
    "    \n",
    "    from_files_len = len(from_files)\n",
    "    to_files_len = len(to_files)\n",
    "    \n",
    "    print(\"Percentile: %d:\\nFrom(%s):%s\\nTo(%s):%s\"% (percent, from_path, from_files_len, to_path, to_files_len))\n",
    "    \n",
    "    return from_files_len, to_files_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data load initiated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/121 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentile: 50:\n",
      "From(data\\val\\NORMAL):121\n",
      "To(data\\test\\NORMAL):121\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/199 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentile: 50:\n",
      "From(data\\val\\PNEUMONIA):199\n",
      "To(data\\test\\PNEUMONIA):199\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(199, 199)"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_norm = r'data\\test\\NORMAL'\n",
    "test_pneu = r'data\\test\\PNEUMONIA'\n",
    "\n",
    "val_norm = r'data\\val\\NORMAL'\n",
    "val_pneu = r'data\\val\\PNEUMONIA'\n",
    "\n",
    "test_norm_files = os.listdir(test_norm)\n",
    "test_pneu_files = os.listdir(test_pneu)\n",
    "\n",
    "print('Data load initiated')\n",
    "\n",
    "move(test_norm, val_norm, 50)\n",
    "move(test_pneu, val_pneu, 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_pred(preds, Y, val_breed, index, seq, ran):\n",
    "    leng = len(preds)\n",
    "    if seq:\n",
    "        for i in range(index):\n",
    "            if ran:\n",
    "                index = random.randint(0, leng) \n",
    "            _, imagenet_class_name, prob = decode_predictions(preds, top=1)[index][0]\n",
    "            plt.title(\"Original: \" + val_breed[Y[index]] + \"\\nPrediction: \" + imagenet_class_name)\n",
    "            plt.imshow(X_train[index])\n",
    "            plt.show()\n",
    "    else:\n",
    "            _, imagenet_class_name, prob = decode_predictions(preds, top=1)[index][0]\n",
    "            plt.title(\"Original: \" + val_breed[Y[index]] + \"\\nPrediction: \" + imagenet_class_name)\n",
    "            plt.imshow(X_train[index])\n",
    "            plt.show()\n",
    "        \n",
    "def accuracy_func(preds, Y, val_breed):\n",
    "    leng = len(preds)\n",
    "    count = 0;\n",
    "    for i in range(leng):\n",
    "        _, imagenet_class_name, prob = decode_predictions(preds, top=1)[i][0]\n",
    "        if val_breed[Y[i]] == imagenet_class_name:\n",
    "            count+=1\n",
    "    accuracy = (count/leng)*100\n",
    "    \n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "cache_dir = expanduser(join('~', '.keras'))\n",
    "if not exists(cache_dir):\n",
    "    makedirs(cache_dir)\n",
    "models_dir = join(cache_dir, 'models')\n",
    "if not exists(models_dir):\n",
    "    makedirs(models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Load the VGG model\n",
    "# vgg_model = vgg16.VGG16(weights='imagenet')\n",
    " \n",
    "# #Load the Inception_V3 model\n",
    "# inception_model = inception_v3.InceptionV3(weights='imagenet')\n",
    " \n",
    "# #Load the ResNet50 model\n",
    "# resnet_model = resnet50.ResNet50(weights='imagenet')\n",
    " \n",
    "# #Load the MobileNet model\n",
    "# mobilenet_model = mobilenet.MobileNet(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_path = 'data\\\\train'\n",
    "validation_path = 'data\\\\val'\n",
    "testing_path = 'data\\\\test'\n",
    "batch_size = 32\n",
    "target_size=(224, 224)\n",
    "norm = 255.0\n",
    "class_mode='categorical'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'norm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-fb8ee6aa2537>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m train_datagen = ImageDataGenerator(\n\u001b[1;32m----> 2\u001b[1;33m         \u001b[0mrescale\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1.\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mshear_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mzoom_range\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0.2\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         horizontal_flip=True)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'norm' is not defined"
     ]
    }
   ],
   "source": [
    "train_datagen = ImageDataGenerator(\n",
    "        rescale=1./norm,\n",
    "        shear_range=0.2,\n",
    "        zoom_range=0.2,\n",
    "        horizontal_flip=True)\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=1./norm)\n",
    "test_datagen = ImageDataGenerator(rescale=1./norm)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "        training_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=class_mode)\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "        validation_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=class_mode)\n",
    "\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "        testing_path,\n",
    "        target_size=target_size,\n",
    "        batch_size=batch_size,\n",
    "        class_mode=class_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the base pre-trained model\n",
    "base_model = InceptionV3(weights='imagenet', include_top=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add a global spatial average pooling layer\n",
    "x = base_model.output\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "# let's add a fully-connected layer\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "# and a logistic layer -- let's say we have 200 classes\n",
    "predictions = Dense(2, activation='softmax')(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is the model we will train\n",
    "model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "# first: train only the top layers (which were randomly initialized)\n",
    "# i.e. freeze all convolutional InceptionV3 layers\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False\n",
    "\n",
    "# compile the model (should be done *after* setting layers to non-trainable)\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 input_3\n",
      "1 conv2d_189\n",
      "2 batch_normalization_189\n",
      "3 activation_189\n",
      "4 conv2d_190\n",
      "5 batch_normalization_190\n",
      "6 activation_190\n",
      "7 conv2d_191\n",
      "8 batch_normalization_191\n",
      "9 activation_191\n",
      "10 max_pooling2d_9\n",
      "11 conv2d_192\n",
      "12 batch_normalization_192\n",
      "13 activation_192\n",
      "14 conv2d_193\n",
      "15 batch_normalization_193\n",
      "16 activation_193\n",
      "17 max_pooling2d_10\n",
      "18 conv2d_197\n",
      "19 batch_normalization_197\n",
      "20 activation_197\n",
      "21 conv2d_195\n",
      "22 conv2d_198\n",
      "23 batch_normalization_195\n",
      "24 batch_normalization_198\n",
      "25 activation_195\n",
      "26 activation_198\n",
      "27 average_pooling2d_19\n",
      "28 conv2d_194\n",
      "29 conv2d_196\n",
      "30 conv2d_199\n",
      "31 conv2d_200\n",
      "32 batch_normalization_194\n",
      "33 batch_normalization_196\n",
      "34 batch_normalization_199\n",
      "35 batch_normalization_200\n",
      "36 activation_194\n",
      "37 activation_196\n",
      "38 activation_199\n",
      "39 activation_200\n",
      "40 mixed0\n",
      "41 conv2d_204\n",
      "42 batch_normalization_204\n",
      "43 activation_204\n",
      "44 conv2d_202\n",
      "45 conv2d_205\n",
      "46 batch_normalization_202\n",
      "47 batch_normalization_205\n",
      "48 activation_202\n",
      "49 activation_205\n",
      "50 average_pooling2d_20\n",
      "51 conv2d_201\n",
      "52 conv2d_203\n",
      "53 conv2d_206\n",
      "54 conv2d_207\n",
      "55 batch_normalization_201\n",
      "56 batch_normalization_203\n",
      "57 batch_normalization_206\n",
      "58 batch_normalization_207\n",
      "59 activation_201\n",
      "60 activation_203\n",
      "61 activation_206\n",
      "62 activation_207\n",
      "63 mixed1\n",
      "64 conv2d_211\n",
      "65 batch_normalization_211\n",
      "66 activation_211\n",
      "67 conv2d_209\n",
      "68 conv2d_212\n",
      "69 batch_normalization_209\n",
      "70 batch_normalization_212\n",
      "71 activation_209\n",
      "72 activation_212\n",
      "73 average_pooling2d_21\n",
      "74 conv2d_208\n",
      "75 conv2d_210\n",
      "76 conv2d_213\n",
      "77 conv2d_214\n",
      "78 batch_normalization_208\n",
      "79 batch_normalization_210\n",
      "80 batch_normalization_213\n",
      "81 batch_normalization_214\n",
      "82 activation_208\n",
      "83 activation_210\n",
      "84 activation_213\n",
      "85 activation_214\n",
      "86 mixed2\n",
      "87 conv2d_216\n",
      "88 batch_normalization_216\n",
      "89 activation_216\n",
      "90 conv2d_217\n",
      "91 batch_normalization_217\n",
      "92 activation_217\n",
      "93 conv2d_215\n",
      "94 conv2d_218\n",
      "95 batch_normalization_215\n",
      "96 batch_normalization_218\n",
      "97 activation_215\n",
      "98 activation_218\n",
      "99 max_pooling2d_11\n",
      "100 mixed3\n",
      "101 conv2d_223\n",
      "102 batch_normalization_223\n",
      "103 activation_223\n",
      "104 conv2d_224\n",
      "105 batch_normalization_224\n",
      "106 activation_224\n",
      "107 conv2d_220\n",
      "108 conv2d_225\n",
      "109 batch_normalization_220\n",
      "110 batch_normalization_225\n",
      "111 activation_220\n",
      "112 activation_225\n",
      "113 conv2d_221\n",
      "114 conv2d_226\n",
      "115 batch_normalization_221\n",
      "116 batch_normalization_226\n",
      "117 activation_221\n",
      "118 activation_226\n",
      "119 average_pooling2d_22\n",
      "120 conv2d_219\n",
      "121 conv2d_222\n",
      "122 conv2d_227\n",
      "123 conv2d_228\n",
      "124 batch_normalization_219\n",
      "125 batch_normalization_222\n",
      "126 batch_normalization_227\n",
      "127 batch_normalization_228\n",
      "128 activation_219\n",
      "129 activation_222\n",
      "130 activation_227\n",
      "131 activation_228\n",
      "132 mixed4\n",
      "133 conv2d_233\n",
      "134 batch_normalization_233\n",
      "135 activation_233\n",
      "136 conv2d_234\n",
      "137 batch_normalization_234\n",
      "138 activation_234\n",
      "139 conv2d_230\n",
      "140 conv2d_235\n",
      "141 batch_normalization_230\n",
      "142 batch_normalization_235\n",
      "143 activation_230\n",
      "144 activation_235\n",
      "145 conv2d_231\n",
      "146 conv2d_236\n",
      "147 batch_normalization_231\n",
      "148 batch_normalization_236\n",
      "149 activation_231\n",
      "150 activation_236\n",
      "151 average_pooling2d_23\n",
      "152 conv2d_229\n",
      "153 conv2d_232\n",
      "154 conv2d_237\n",
      "155 conv2d_238\n",
      "156 batch_normalization_229\n",
      "157 batch_normalization_232\n",
      "158 batch_normalization_237\n",
      "159 batch_normalization_238\n",
      "160 activation_229\n",
      "161 activation_232\n",
      "162 activation_237\n",
      "163 activation_238\n",
      "164 mixed5\n",
      "165 conv2d_243\n",
      "166 batch_normalization_243\n",
      "167 activation_243\n",
      "168 conv2d_244\n",
      "169 batch_normalization_244\n",
      "170 activation_244\n",
      "171 conv2d_240\n",
      "172 conv2d_245\n",
      "173 batch_normalization_240\n",
      "174 batch_normalization_245\n",
      "175 activation_240\n",
      "176 activation_245\n",
      "177 conv2d_241\n",
      "178 conv2d_246\n",
      "179 batch_normalization_241\n",
      "180 batch_normalization_246\n",
      "181 activation_241\n",
      "182 activation_246\n",
      "183 average_pooling2d_24\n",
      "184 conv2d_239\n",
      "185 conv2d_242\n",
      "186 conv2d_247\n",
      "187 conv2d_248\n",
      "188 batch_normalization_239\n",
      "189 batch_normalization_242\n",
      "190 batch_normalization_247\n",
      "191 batch_normalization_248\n",
      "192 activation_239\n",
      "193 activation_242\n",
      "194 activation_247\n",
      "195 activation_248\n",
      "196 mixed6\n",
      "197 conv2d_253\n",
      "198 batch_normalization_253\n",
      "199 activation_253\n",
      "200 conv2d_254\n",
      "201 batch_normalization_254\n",
      "202 activation_254\n",
      "203 conv2d_250\n",
      "204 conv2d_255\n",
      "205 batch_normalization_250\n",
      "206 batch_normalization_255\n",
      "207 activation_250\n",
      "208 activation_255\n",
      "209 conv2d_251\n",
      "210 conv2d_256\n",
      "211 batch_normalization_251\n",
      "212 batch_normalization_256\n",
      "213 activation_251\n",
      "214 activation_256\n",
      "215 average_pooling2d_25\n",
      "216 conv2d_249\n",
      "217 conv2d_252\n",
      "218 conv2d_257\n",
      "219 conv2d_258\n",
      "220 batch_normalization_249\n",
      "221 batch_normalization_252\n",
      "222 batch_normalization_257\n",
      "223 batch_normalization_258\n",
      "224 activation_249\n",
      "225 activation_252\n",
      "226 activation_257\n",
      "227 activation_258\n",
      "228 mixed7\n",
      "229 conv2d_261\n",
      "230 batch_normalization_261\n",
      "231 activation_261\n",
      "232 conv2d_262\n",
      "233 batch_normalization_262\n",
      "234 activation_262\n",
      "235 conv2d_259\n",
      "236 conv2d_263\n",
      "237 batch_normalization_259\n",
      "238 batch_normalization_263\n",
      "239 activation_259\n",
      "240 activation_263\n",
      "241 conv2d_260\n",
      "242 conv2d_264\n",
      "243 batch_normalization_260\n",
      "244 batch_normalization_264\n",
      "245 activation_260\n",
      "246 activation_264\n",
      "247 max_pooling2d_12\n",
      "248 mixed8\n",
      "249 conv2d_269\n",
      "250 batch_normalization_269\n",
      "251 activation_269\n",
      "252 conv2d_266\n",
      "253 conv2d_270\n",
      "254 batch_normalization_266\n",
      "255 batch_normalization_270\n",
      "256 activation_266\n",
      "257 activation_270\n",
      "258 conv2d_267\n",
      "259 conv2d_268\n",
      "260 conv2d_271\n",
      "261 conv2d_272\n",
      "262 average_pooling2d_26\n",
      "263 conv2d_265\n",
      "264 batch_normalization_267\n",
      "265 batch_normalization_268\n",
      "266 batch_normalization_271\n",
      "267 batch_normalization_272\n",
      "268 conv2d_273\n",
      "269 batch_normalization_265\n",
      "270 activation_267\n",
      "271 activation_268\n",
      "272 activation_271\n",
      "273 activation_272\n",
      "274 batch_normalization_273\n",
      "275 activation_265\n",
      "276 mixed9_0\n",
      "277 concatenate_5\n",
      "278 activation_273\n",
      "279 mixed9\n",
      "280 conv2d_278\n",
      "281 batch_normalization_278\n",
      "282 activation_278\n",
      "283 conv2d_275\n",
      "284 conv2d_279\n",
      "285 batch_normalization_275\n",
      "286 batch_normalization_279\n",
      "287 activation_275\n",
      "288 activation_279\n",
      "289 conv2d_276\n",
      "290 conv2d_277\n",
      "291 conv2d_280\n",
      "292 conv2d_281\n",
      "293 average_pooling2d_27\n",
      "294 conv2d_274\n",
      "295 batch_normalization_276\n",
      "296 batch_normalization_277\n",
      "297 batch_normalization_280\n",
      "298 batch_normalization_281\n",
      "299 conv2d_282\n",
      "300 batch_normalization_274\n",
      "301 activation_276\n",
      "302 activation_277\n",
      "303 activation_280\n",
      "304 activation_281\n",
      "305 batch_normalization_282\n",
      "306 activation_274\n",
      "307 mixed9_1\n",
      "308 concatenate_6\n",
      "309 activation_282\n",
      "310 mixed10\n"
     ]
    }
   ],
   "source": [
    "# at this point, the top layers are well trained and we can start fine-tuning\n",
    "# convolutional layers from inception V3. We will freeze the bottom N layers\n",
    "# and train the remaining top layers.\n",
    "\n",
    "# let's visualize layer names and layer indices to see how many layers\n",
    "# we should freeze:\n",
    "for i, layer in enumerate(base_model.layers):\n",
    "    print(i, layer.name)\n",
    "\n",
    "# we chose to train the top 2 inception blocks, i.e. we will freeze\n",
    "# the first 249 layers and unfreeze the rest:\n",
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to recompile the model for these modifications to take effect\n",
    "# we use SGD with a low learning rate\n",
    "\n",
    "sgd = optimizers.Adam()\n",
    "# sgd = optimizers.SGD()\n",
    "# sgd = optimizers.SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "# sgd = optimizer=SGD(lr=0.0001, momentum=0.9)\n",
    "\n",
    "model.compile(sgd, loss='categorical_crossentropy', metrics=[\"accuracy\"])\n",
    "\n",
    "# model.compile(loss='mean_squared_error', optimizer='sgd')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = 'outputs/models/'\n",
    "log_file = \"outputs/logs\"\n",
    "\n",
    "model_file = model_dir+\"weights-improvement-{epoch:02d}-{val_acc:.2f}.hdf5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = keras.callbacks.ModelCheckpoint(model_file, monitor='val_acc', verbose=0, save_best_only=False, save_weights_only=False, mode='auto', period=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None)\n",
    "# early_stopping = keras.callbacks.EarlyStopping(monitor='val_acc', min_delta=0, patience=0, verbose=0, mode='auto', baseline=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensorboard = keras.callbacks.TensorBoard(log_dir=log_file, histogram_freq=0, batch_size=32, write_graph=True, write_grads=False, write_images=False, embeddings_freq=0, embeddings_layer_names=None, embeddings_metadata=None, embeddings_data=None)\n",
    "tensorboard.set_model(model) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callbacks_list = [checkpoint, tensorboard, early_stopping]\n",
    "callbacks_list = [checkpoint, tensorboard]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_dir(directory):\n",
    "    if os.path.exists(directory):\n",
    "        try:\n",
    "            shutil.rmtree(directory)\n",
    "            os.mkdir(directory)\n",
    "        except:\n",
    "            print(\"error:\", directory)\n",
    "    else:\n",
    "        try:\n",
    "            os.mkdir(directory)\n",
    "        except:\n",
    "            print(\"error create:\", directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_dir(model_dir)\n",
    "clear_dir(log_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=50,\n",
    "    validation_data=validation_generator,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training & validation accuracy values\n",
    "plt.plot(history.history['acc'])\n",
    "plt.plot(history.history['val_acc'])\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation loss values\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(['Train', 'Val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "filepath = 'outputs/models/weights-improvement-01-0.92.hdf5'\n",
    "model = keras.models.load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we train our model again (this time fine-tuning the top 2 inception blocks\n",
    "# alongside the top Dense layers\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    epochs=15,\n",
    "    validation_data=validation_generator,\n",
    "    verbose=1,\n",
    "    callbacks=callbacks_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = 'outputs/models/weights-improvement-01-0.65.hdf5'\n",
    "# filepath = 'outputs/models/weights-improvement-02-0.66.hdf5'\n",
    "# filepath = 'outputs/models/weights-improvement-03-0.88.hdf5'\n",
    "filepath = 'outputs/models/weights-improvement-04-0.75.hdf5'\n",
    "\n",
    "\n",
    "# filepath = 'outputs/models/weights-improvement-01-0.92.hdf5'\n",
    "# filepath = 'outputs/models/weights-improvement-02-0.83.hdf5'\n",
    "\n",
    "model = keras.models.load_model(filepath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 3: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 53s - ETA: 39 - ETA: 25 - ETA: 2: - 1569s 157s/step\n"
     ]
    }
   ],
   "source": [
    "result = model.evaluate_generator(generator=test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 75.31%\n",
      "Loss: 0.47\n"
     ]
    }
   ],
   "source": [
    "print(\"%s%.2f%s\"% (\"Accuracy: \", result[1]*100, \"%\"))\n",
    "print(\"%s%.2f\"% (\"Loss: \", result[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10/10 [==============================] - ETA: 2: - ETA: 2: - ETA: 1: - ETA: 1: - ETA: 1: - ETA: 58s - ETA: 42 - ETA: 28 - ETA: 14 - 142s 14s/step\n"
     ]
    }
   ],
   "source": [
    "preds = model.predict_generator(test_generator, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_accuracy = keras.metrics.categorical_accuracy(test_generator.classes, preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'Cast_2:0' shape=(320,) dtype=float32>"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "categorical_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "filepath = 'outputs/models/weights-improvement-01-0.66.hdf5'\n",
    "\n",
    "\n",
    "filepath = 'outputs/models/weights-improvement-02-0.68.hdf5'\n",
    "\n",
    "\n",
    "filepath = 'outputs/models/weights-improvement-03-0.88.hdf5'\n",
    "\n",
    "\n",
    "filepath = 'outputs/models/weights-improvement-04-0.75.hdf5'\n",
    "Accuracy: 75.31%\n",
    "Loss: 0.47\n",
    "\n",
    "\n",
    "filepath = 'outputs/models/weights-improvement-01-0.92.hdf5'\n",
    "Accuracy: 80.94%\n",
    "Loss: 0.47\n",
    "'''\n",
    "# tensorboard --logdir=logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_class_values(gen):\n",
    "    vals = []\n",
    "    for i in gen:\n",
    "        vals.append(gen[i])\n",
    "    return vals\n",
    "\n",
    "def get_classes(gen):\n",
    "    rev_gen = {}\n",
    "    for i in gen:\n",
    "        rev_gen[gen[i]] = i\n",
    "    return rev_gen\n",
    "\n",
    "def find_max(preds_i):\n",
    "    num = -1\n",
    "    index = -1\n",
    "    for i in range(len(preds_i)):\n",
    "        if preds_i[i]>num:\n",
    "            num = preds_i[i]\n",
    "            index = i\n",
    "    return index  \n",
    "\n",
    "def get_label(cls, val):\n",
    "    return cls[val]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vals = get_class_values(test_generator.class_indices)\n",
    "cls = get_classes(test_generator.class_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total = 0\n",
    "acc = 0\n",
    "direc = \"data\\\\pre\\\\test\"\n",
    "files = test_generator.filenames\n",
    "for i in range(len(preds)):\n",
    "    cls_index = find_max(preds[i])\n",
    "    pred_label = cls[cls_index]\n",
    "    org_label = cls[test_generator.classes[i]]\n",
    "    file_name = os.path.join(direc, files[i])\n",
    "    if org_label==pred_label:\n",
    "        acc +=1\n",
    "    if total%10==0:\n",
    "        result = \"Wrong!!!\\n\"\n",
    "        if org_label==pred_label:\n",
    "            result = \"Right!!!\\n\"\n",
    "        title_text = \"Original: \"+ org_label+\"\\nPredicted: \"+ pred_label+\"\\nFile: \"+ file_name\n",
    "        title_text=result+ title_text\n",
    "        \n",
    "        img = cv2.imread(file_name, 3)\n",
    "        if org_label==pred_label:\n",
    "            plt.title(title_text, color='blue')\n",
    "        else:\n",
    "            plt.title(title_text, color='red')\n",
    "        plt.imshow(img)\n",
    "        plt.show()\n",
    "    \n",
    "    total+=1\n",
    "accuracy = (acc/total)*100\n",
    "print(\"Accuracy: \", accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
