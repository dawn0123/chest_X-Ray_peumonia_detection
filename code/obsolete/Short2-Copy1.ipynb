{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import random\n",
    "\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "from collections import Counter\n",
    "import numpy as np\n",
    "\n",
    "import shutil\n",
    "import inspect\n",
    "\n",
    "import gc\n",
    "\n",
    "import re\n",
    "\n",
    "import keras\n",
    "from keras import models\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.applications.inception_v3 import InceptionV3\n",
    "from keras.layers import Dense, Dropout, GlobalAveragePooling2D, GlobalAveragePooling1D, Flatten, BatchNormalization\n",
    "from keras import optimizers\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping, TensorBoard, ReduceLROnPlateau\n",
    "\n",
    "from keras import backend as K\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "from IPython.display import display\n",
    "from PIL import Image\n",
    "\n",
    "import seaborn as sns\n",
    "from matplotlib.pyplot import figure\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "%matplotlib inline\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from mlxtend.plotting import plot_confusion_matrix\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates directory, if directory exists removes if remove parameter is set to True \n",
    "def create_directory(directory_path, remove=False):\n",
    "    if remove and os.path.exists(directory_path):\n",
    "        try:\n",
    "            shutil.rmtree(directory_path)\n",
    "            os.mkdir(directory_path)\n",
    "        except:\n",
    "            print(\"Could not remove directory : \", directory_path)\n",
    "            return False\n",
    "    else:\n",
    "        try:\n",
    "            os.mkdir(directory_path)\n",
    "        except:\n",
    "            print(\"Could not create directory: \", directory_path)\n",
    "            return False\n",
    "        \n",
    "    return True\n",
    "\n",
    "# Removes directory, if directory exists \n",
    "def remove_directory(directory_path):\n",
    "    if os.path.exists(directory_path):\n",
    "        try:\n",
    "            shutil.rmtree(directory_path)\n",
    "        except:\n",
    "            print(\"Could not remove directory : \", directory_path)\n",
    "            return False\n",
    "        \n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print date and time for given type of representation\n",
    "def date_time(x):\n",
    "    if x==1:\n",
    "        print('Timestamp: {:%Y-%m-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "    if x==2:    \n",
    "        print('Timestamp: {:%Y-%b-%d %H:%M:%S}'.format(datetime.datetime.now()))\n",
    "    if x==3:  \n",
    "        print('Date now: %s' % datetime.datetime.now())\n",
    "    if x==4:  \n",
    "        print('Date today: %s' % datetime.date.today())  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prints a integer for degugging\n",
    "def debug(x):\n",
    "    print(\"-\"*40, x, \"-\"*40)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removes everything except alphabetical and selected characters from name string\n",
    "def name_correct(name):\n",
    "    return re.sub(r'[^a-zA-Z,:]', ' ', name).title()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset tensorflow graph tp free up memory and resource allocation \n",
    "def reset_graph(model=None):\n",
    "    try:\n",
    "        del model\n",
    "    except:\n",
    "        return False\n",
    "    \n",
    "    tf.reset_default_graph()\n",
    "    \n",
    "    K.clear_session()\n",
    "    \n",
    "    gc.collect()\n",
    "    \n",
    "    return True\n",
    "\n",
    "\n",
    "# reset callbacks \n",
    "def reset_callbacks(checkpoint=None, reduce_lr=None, early_stopping=None, tensorboard=None):\n",
    "    checkpoint=None\n",
    "    reduce_lr = None\n",
    "    early_stopping = None\n",
    "    tensorboard = None\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-24a8d2cd135d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreset_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mreset_callbacks\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "reset_graph(model)\n",
    "reset_callbacks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NORMAL', 'PNEUMONIA']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['NORMAL', 'PNEUMONIA']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Configure input/ output directory\n",
    "# Configure training, validation, testing directory\n",
    "\n",
    "input_directory = r\"data/input/\"\n",
    "output_directory = r\"data/output/\"\n",
    "\n",
    "training_dir = input_directory+ r\"train\"\n",
    "validation_dir = input_directory+ r\"val\"\n",
    "testing_dir = input_directory+ r\"test\"\n",
    "\n",
    "classes = os.listdir(training_dir)\n",
    "print(classes)\n",
    "classes = ['NORMAL', 'PNEUMONIA']\n",
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5216 images belonging to 2 classes.\n",
      "Found 320 images belonging to 2 classes.\n",
      "Found 320 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "target_size = (299,299)\n",
    "# target_size = (150,150)\n",
    "\n",
    "rescale=1./255.0\n",
    "# batch_size = 32\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "\n",
    "class_mode='categorical'\n",
    "\n",
    "datagen = ImageDataGenerator(\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    shear_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    rescale=rescale)     \n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    training_dir,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "\n",
    "validation_datagen = ImageDataGenerator(rescale=rescale)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=batch_size)    \n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=rescale)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    testing_dir,\n",
    "    target_size=target_size,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.utils import class_weight\n",
    "\n",
    "# class_weight = class_weight.compute_class_weight('balanced', np.unique(train_generator.classes), train_generator.classes)\n",
    "# class_weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "# Model Directory\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "model_dir=output_directory + r\"models/\"+time.strftime('%Y-%m-%d %H-%M-%S')+\"/\"\n",
    "# model_dir=output_directory + r\"models/\"+time.strftime('%Y%m%d%H%M%S')+\"/\"\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "# Log Directory\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "# log_dir=output_directory + r\"logs/\"+time.strftime('%Y%m%d%H%M%S')\n",
    "log_dir=output_directory + r\"logs/\"+time.strftime('%Y-%m-%d %H-%M-%S')\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "\n",
    "# Create Output Directory (Model and Log)\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "create_directory(model_dir, remove=True)\n",
    "create_directory(log_dir, remove=True)\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "# Model File Name Configuration\n",
    "#------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "# Initial Trainning Model Filename \n",
    "#----------------------------------------------------------------------------------------------------#\n",
    "init_model_file=model_dir+\"base-\"+\"{epoch:02d}-val_acc-{val_acc:.2f}-val_loss-{val_loss:.2f}.hdf5\"\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "# Trainning Model Filename \n",
    "#----------------------------------------------------------------------------------------------------#\n",
    "model_file=model_dir+\"{epoch:02d}-val_acc-{val_acc:.2f}-val_loss-{val_loss:.2f}.hdf5\"\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "\n",
    "#--------------------------------------------------------------------------------------------------------------------#\n",
    "# Retrainning Model Filename \n",
    "#----------------------------------------------------------------------------------------------------#\n",
    "retrain_model_file=model_dir+\"retrain-{epoch:02d}-val_acc-{val_acc:.2f}-val_loss-{val_loss:.2f}.hdf5\"\n",
    "#--------------------------------------------------------------------------------------------------------------------#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-12-455a2dc4cf83>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mreset_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'model' is not defined"
     ]
    }
   ],
   "source": [
    "reset_graph(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    base_model = InceptionV3(weights='imagenet', include_top=False)\n",
    "\n",
    "    x = base_model.output\n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    #     x = Dropout(0.5)(x)\n",
    "    x = Dense(128, activation='relu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    predictions = Dense(2, activation='softmax')(x) \n",
    "\n",
    "    model = Model(inputs=base_model.input, outputs=predictions)\n",
    "\n",
    "\n",
    "    for layer in base_model.layers:\n",
    "        layer.trainable = False\n",
    "\n",
    "    model.summary()\n",
    "    \n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint(\n",
    "    model_file, \n",
    "    monitor='val_loss',\n",
    "    verbose=0,\n",
    "    save_best_only=True, \n",
    "    save_weights_only=False, \n",
    "    mode='auto',\n",
    "    period=1)\n",
    "\n",
    "# early_stopping = EarlyStopping(\n",
    "#     monitor='val_loss',\n",
    "#     min_delta=0, \n",
    "#     patience=0, \n",
    "#     verbose=0, \n",
    "#     mode='auto', \n",
    "#     baseline=None, \n",
    "#     restore_best_weights=False)\n",
    "\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_loss', \n",
    "    patience=10, \n",
    "    verbose=1,\n",
    "    restore_best_weights=True)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "tensorboard = TensorBoard(\n",
    "    log_dir=log_dir,\n",
    "    batch_size=batch_size)\n",
    "\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "# from keras.callbacks import Callback\n",
    "    \n",
    "    \n",
    "# from keras import backend as K\n",
    "\n",
    "# def check_units(y_true, y_pred):\n",
    "#     if y_pred.shape[1] != 1:\n",
    "#         y_pred = y_pred[:,1:2]\n",
    "#         y_true = y_true[:,1:2]\n",
    "#     return y_true, y_pred\n",
    "\n",
    "# def precision(y_true, y_pred):\n",
    "#     y_true, y_pred = check_units(y_true, y_pred)\n",
    "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "#     precision = true_positives / (predicted_positives + K.epsilon())\n",
    "#     return precision\n",
    "\n",
    "# def recall(y_true, y_pred):\n",
    "#     y_true, y_pred = check_units(y_true, y_pred)\n",
    "#     true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#     possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "#     recall = true_positives / (possible_positives + K.epsilon())\n",
    "#     return recall\n",
    "\n",
    "\n",
    "# def f1(y_true, y_pred):\n",
    "#     def recall(y_true, y_pred):\n",
    "#         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#         possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "#         recall = true_positives / (possible_positives + K.epsilon())\n",
    "#         return recall\n",
    "\n",
    "#     def precision(y_true, y_pred):\n",
    "#         true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "#         predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "#         precision = true_positives / (predicted_positives + K.epsilon())\n",
    "#         return precision\n",
    "#     y_true, y_pred = check_units(y_true, y_pred)\n",
    "#     precision = precision(y_true, y_pred)\n",
    "#     recall = recall(y_true, y_pred)\n",
    "#     return 2*((precision*recall)/(precision+recall+K.epsilon()))\n",
    "\n",
    "\n",
    "\n",
    "# def macro_f1(y_true, y_pred):\n",
    "#     y_pred = K.round(y_pred)\n",
    "#     tp = K.sum(K.cast(y_true*y_pred, 'float'), axis=0)\n",
    "#     # tn = K.sum(K.cast((1-y_true)*(1-y_pred), 'float'), axis=0)\n",
    "#     fp = K.sum(K.cast((1-y_true)*y_pred, 'float'), axis=0)\n",
    "#     fn = K.sum(K.cast(y_true*(1-y_pred), 'float'), axis=0)\n",
    "\n",
    "#     p = tp / (tp + fp + K.epsilon())\n",
    "#     r = tp / (tp + fn + K.epsilon())\n",
    "\n",
    "#     f1 = 2*p*r / (p+r+K.epsilon())\n",
    "#     f1 = tf.where(tf.is_nan(f1), tf.zeros_like(f1), f1)\n",
    "#     return K.mean(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# from keras.callbacks import Callback\n",
    "# from sklearn.metrics import confusion_matrix, f1_score, precision_score, recall_score\n",
    "# class Metrics(Callback):\n",
    "#     def on_train_begin(self, logs={}):\n",
    "#         self.val_f1s = []\n",
    "#         self.val_recalls = []\n",
    "#         self.val_precisions = []\n",
    "\n",
    "#     def on_epoch_end(self, epoch, logs={}):\n",
    "#         val_predict = (np.asarray(self.model.predict(self.model.validation_data[0]))).round()\n",
    "#         val_targ = self.model.validation_data[1]\n",
    "#         _val_f1 = f1_score(val_targ, val_predict)\n",
    "#         _val_recall = recall_score(val_targ, val_predict)\n",
    "#         _val_precision = precision_score(val_targ, val_predict)\n",
    "#         self.val_f1s.append(_val_f1)\n",
    "#         self.val_recalls.append(_val_recall)\n",
    "#         self.val_precisions.append(_val_precision)\n",
    "#         print (\" — val_f1: %f — val_precision: %f — val_recall %f\" %(_val_f1, _val_precision, _val_recall))\n",
    "#         return\n",
    "\n",
    "# metrics = Metrics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time(1)\n",
    "\n",
    "# callbacks = [checkpoint, reduce_lr, early_stopping, tensorboard]\n",
    "# callbacks = [checkpoint, reduce_lr, early_stopping, tensorboard]\n",
    "callbacks = [checkpoint, reduce_lr, early_stopping, tensorboard]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time(1)\n",
    "model = model()\n",
    "date_time(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer=optimizers.Adam()\n",
    "loss='categorical_crossentropy'\n",
    "# metrics=['accuracy']\n",
    "\n",
    "\n",
    "# model.compile(optimizer, loss=loss, metrics=metrics)\n",
    "model.compile(loss=loss,\n",
    "              optimizer= optimizer,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "date_time(1)\n",
    "\n",
    "history = model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = len(train_generator),\n",
    "    epochs=30,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    class_weight=class_weight)\n",
    "\n",
    "\n",
    "date_time(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for layer in model.layers[:249]:\n",
    "    layer.trainable = False\n",
    "for layer in model.layers[249:]:\n",
    "    layer.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time(1)\n",
    "\n",
    "cur_model_file = r\"data\\output\\models\\2018-12-08 17-16-53\\03-val_acc-0.94-val_loss-0.16.hdf5\"\n",
    "\n",
    "cur_model = keras.models.load_model(cur_model_file, custom_objects={'accuracy':'accuracy', 'precision':precision, 'recall':recall, 'f1':f1})\n",
    "\n",
    "# cur_model = keras.models.load_model(cur_model_file)\n",
    "optimizer=optimizers.Adam(lr=0.00001)\n",
    "loss='categorical_crossentropy'\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "metrics=['accuracy', precision, recall, f1_score]\n",
    "cur_model.compile(optimizer, loss=loss, metrics=metrics)\n",
    "\n",
    "cur_model.compile(\n",
    "    loss=loss,\n",
    "    optimizer= optimizer,\n",
    "    metrics=metrics)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "history = cur_model.fit_generator(\n",
    "    train_generator,\n",
    "    steps_per_epoch = len(train_generator),\n",
    "    epochs=30,\n",
    "    callbacks=callbacks,\n",
    "    validation_data=validation_generator,\n",
    "    validation_steps=len(validation_generator),\n",
    "    class_weight=class_weight)\n",
    "\n",
    "\n",
    "date_time(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"data/output/models/\"\n",
    "\n",
    "dirs = os.listdir(model_dir)\n",
    "for i in range(len(dirs)):\n",
    "    print(i, dirs[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_dir = model_dir+dirs[2]+\"/\"\n",
    "\n",
    "print(current_dir)\n",
    "cur_models = os.listdir(current_dir)\n",
    "for i in range(len(cur_models)):\n",
    "    print(i, cur_models[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_model = cur_models[5]\n",
    "model_file = current_dir+cur_model\n",
    "print(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_generator = validation_datagen.flow_from_directory(\n",
    "    validation_dir,\n",
    "    target_size=target_size,\n",
    "    classes = classes,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=1)\n",
    "\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_directory(\n",
    "    testing_dir,\n",
    "    target_size=target_size,\n",
    "    classes = classes,\n",
    "    class_mode=class_mode,\n",
    "    batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.load_model(model_file, custom_objects={'accuracy':'accuracy', 'precision':precision, 'recall':recall, 'f1':f1})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time(1)\n",
    "# result = model.evaluate_generator(generator=test_generator, steps=100, verbose=1)\n",
    "result = model.evaluate_generator(generator=validation_generator, steps=len(validation_generator), verbose=1)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = result[0]\n",
    "accuracy = result[1]*100\n",
    "precision = result[2]*100\n",
    "recall = result[3]*100\n",
    "f1_score = result[4]*100\n",
    "\n",
    "print(\"%s%.2f%s\"% (\"Accuracy: \", accuracy, \"%\"))\n",
    "print(\"%s%.2f%s\"% (\"Precision: \", precision, \"%\"))\n",
    "print(\"%s%.2f%s\"% (\"Recall: \", recall, \"%\"))\n",
    "print(\"%s%.2f%s\"% (\"F1 - score: \", f1_score, \"%\"))\n",
    "print(\"%s%.2f\"% (\"Loss: \", loss))\n",
    "\n",
    "date_time(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_time(1)\n",
    "\n",
    "# y_preds = model.predict_generator(test_generator, steps=len(test_generator), verbose=1)\n",
    "y_preds = model.predict_generator(test_generator, steps=len(test_generator), verbose=1)\n",
    "        \n",
    "y_classes = y_preds.argmax(axis=-1)\n",
    "date_time(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = confusion_matrix(test_generator.classes, y_classes)\n",
    "\n",
    "fig, ax = plot_confusion_matrix(\n",
    "    conf_mat=CM ,  \n",
    "    figsize=(10,8), \n",
    "    hide_ticks=True,cmap=plt.cm.Blues)\n",
    "# plt.title(\"Model File: \"+cur_model)\n",
    "plt.xticks(range(len(classes)), classes, fontsize=14)\n",
    "plt.yticks(range(len(classes)), classes, fontsize=14)\n",
    "# cm_name = \"data/output/figures/CM-\"+cur_model[:-5]+\".png\"\n",
    "# print(cm_name)\n",
    "# plt.savefig(\"data/output/figures/CM\"+cur_model+\".png\", dpi=None, facecolor='w', edgecolor='w',\n",
    "#         orientation='portrait', papertype=None, format=None,\n",
    "#         transparent=False, bbox_inches=None, pad_inches=0.1,\n",
    "#         frameon=None, metadata=None)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM = confusion_matrix(test_generator.classes, y_classes)\n",
    "\n",
    "cls_report_print = classification_report(test_generator.classes, y_classes, target_names=classes)\n",
    "\n",
    "# cls_report = classification_report(test_generator.classes, y_classes, target_names=classes, output_dict=True)\n",
    "\n",
    "# if print_report: \n",
    "print(cls_report_print)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "y_true=test_generator.classes\n",
    "y_pred = y_preds.argmax(-1) \n",
    "\n",
    "x=metrics.accuracy_score(y_true, y_pred)\n",
    "\n",
    "x=metrics.accuracy_score(y_true, y_pred, normalize=False)\n",
    "\n",
    "fpr, tpr, thresholds = metrics.roc_curve(y_true, y_pred, pos_label=2)\n",
    "\n",
    "x=metrics.auc(fpr, tpr)\n",
    "\n",
    "\n",
    "x=metrics.average_precision_score(y_true, y_pred) \n",
    "\n",
    "\n",
    "x=metrics.balanced_accuracy_score(y_true, y_pred)\n",
    "\n",
    "x=metrics.brier_score_loss(y_true, 1-y_pred, pos_label=1) \n",
    "\n",
    "x=metrics.f1_score(y_true, y_pred, average='micro')\n",
    "print(x)\n",
    "x=metrics.f1_score(y_true, y_pred, average='macro')\n",
    "x=metrics.f1_score(y_true, y_pred, average='weighted')\n",
    "x=metrics.f1_score(y_true, y_pred, average=None)\n",
    "\n",
    "\n",
    "\n",
    "# x=metrics.fbeta_score(y_true, y_pred, average='macro', beta=0.5)\n",
    "\n",
    "# x=metrics.fbeta_score(y_true, y_pred, average='micro', beta=0.5)\n",
    "\n",
    "# x=metrics.fbeta_score(y_true, y_pred, average='weighted', beta=0.5)\n",
    "\n",
    "# x=metrics.fbeta_score(y_true, y_pred, average=None, beta=0.5)\n",
    "\n",
    "# x=metrics.hamming_loss(y_true, y_pred)\n",
    "\n",
    "x=metrics.roc_auc_score(y_true, y_pred)\n",
    "print(x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
